{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nickel Price Prediction: Pipeline (Fixed Val/Test)\n",
    "**Flow**\n",
    "1. ë°ì´í„° íŠ¹ì„±/ëª©í‘œ/EDA + ì „ì²˜ë¦¬\n",
    "2. SHAP ê¸°ë°˜ í”¼ì²˜ ì…€ë ‰ì…˜ (ê¸ˆì† + ê²½ì œì§€í‘œ)\n",
    "3. Baseline -> Residual -> ROR **ìŠ¤íƒ ì¡°í•© ê²€ìƒ‰** (XGB/LGBM/Cat/GBM/Ada)\n",
    "4. Validation/Test ë¶„ë¦¬ ê³ ì • (Val: 2025-08-04~2025-10-20, Test: 2025-10-27~2026-01-12)\n",
    "5. ROR ë°±í…ŒìŠ¤íŠ¸ + ë°©í–¥ì„± í‰ê°€\n",
    "6. Transformer baseline (ë³„ë„ ë…¸íŠ¸ë¶: dl_lstm_transformer.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Experiment Design\n",
    "ëª©í‘œ: ëˆ„ìˆ˜ ì—†ëŠ” í‰ê°€ì™€ ê³µì •í•œ ëª¨ë¸ ë¹„êµ.\n",
    "- yëŠ” **ë‹ˆì¼ˆ ê°€ê²© ë ˆë²¨**ë¡œ ê³ ì •\n",
    "- xëŠ” **1ì£¼ ì§€ì—°ëœ ìˆ˜ìµë¥ /ì°¨ë¶„ í”¼ì²˜** (ëˆ„ìˆ˜ ë°©ì§€)\n",
    "- Split ê³ ì •:\n",
    "  - Train: 2025-08-04 ì´ì „\n",
    "  - Validation: 2025-08-04 ~ 2025-10-20\n",
    "  - Test: 2025-10-27 ~ 2026-01-12\n",
    "- ëª¨ë¸ ì„ íƒ/íŠœë‹ì€ Validationì—ì„œë§Œ, TestëŠ” ìµœì¢… í‰ê°€ìš©\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ ì¡°í•© ì‹¤í—˜ ì„¤ê³„í‘œ (Stage-wise ì¡°í•© ì„¤ê³„)\n",
    "\n",
    "## ğŸ”¹ Stage ì •ì˜ (ê³ ì •)\n",
    "\n",
    "* **Stage 1 (Baseline)**\n",
    "  ì›ë³¸ X â†’ ë‹ˆì¼ˆ ê°€ê²© ì˜ˆì¸¡\n",
    "\n",
    "* **Stage 2 (Residual)**\n",
    "  residual = ì‹¤ì œ âˆ’ baseline ì˜ˆì¸¡\n",
    "  â†’ residual ë³´ì •\n",
    "\n",
    "* **Stage 3 (RoR)**\n",
    "  ror = baseline ì˜ˆì¸¡ âˆ’ residual\n",
    "  â†’ ìµœì¢… ë¯¸ì„¸ ë³´ì •\n",
    "\n",
    "ğŸ‘‰ **StageëŠ” ê³ ì •, ëª¨ë¸ë§Œ ë°”ë€ë‹¤**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ì‚¬ìš© ëª¨ë¸ í’€ (ì¶•ì†Œëœ í›„ë³´)\n",
    "\n",
    "| êµ¬ë¶„ | ëª¨ë¸       | ì—­í•  íŠ¹ì„±         |\n",
    "| -- | -------- | ------------- |\n",
    "| B1 | XGBoost  | í° êµ¬ì¡°, ê°•í•œ ë¹„ì„ í˜•  |\n",
    "| B2 | LightGBM | ì”ì°¨ ì•ˆì •í™”, ë¹ ë¦„    |\n",
    "| B3 | CatBoost | ê³¼ì í•© ì–µì œ, ë¯¸ì„¸ ì¡°ì • |\n",
    "\n",
    "ğŸ‘‰ AdaBoost, GBMì€ **Baseline ì˜ˆë¹„ ì‹¤í—˜ìš©**ìœ¼ë¡œë§Œ ì‚¬ìš©\n",
    "ğŸ‘‰ ë³¸ ì‹¤í—˜ ì¡°í•©ì—ì„œëŠ” ì œì™¸ (ì‹¤í—˜ ìˆ˜ í­ë°œ ë°©ì§€)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 1ë‹¨ê³„: Baseline ë‹¨ì¼ ëª¨ë¸ ë¹„êµ\n",
    "\n",
    "| ì‹¤í—˜ ID | Baseline | Residual | RoR |\n",
    "| ----- | -------- | -------- | --- |\n",
    "| BL-1  | XGBoost  | â€“        | â€“   |\n",
    "| BL-2  | LightGBM | â€“        | â€“   |\n",
    "| BL-3  | CatBoost | â€“        | â€“   |\n",
    "\n",
    "ëª©ì :\n",
    "\n",
    "* **â€œì–´ë–¤ ëª¨ë¸ì´ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ê°€ì¥ ì˜ ì¡ëŠ”ê°€â€**\n",
    "* ìƒìœ„ 2ê°œë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2ë‹¨ê³„: Baseline + Residual ì¡°í•©\n",
    "\n",
    "(â€» Baseline ìƒìœ„ 2ê°œë§Œ ì‚¬ìš©)\n",
    "\n",
    "| ì‹¤í—˜ ID | Baseline | Residual | RoR |\n",
    "| ----- | -------- | -------- | --- |\n",
    "| BR-1  | XGBoost  | LightGBM | â€“   |\n",
    "| BR-2  | XGBoost  | CatBoost | â€“   |\n",
    "| BR-3  | LightGBM | LightGBM | â€“   |\n",
    "| BR-4  | LightGBM | CatBoost | â€“   |\n",
    "\n",
    "ëª©ì :\n",
    "\n",
    "* Residual ë‹¨ê³„ì—ì„œ **ì–´ë–¤ ëª¨ë¸ì´ ì˜¤ì°¨ êµ¬ì¡°ë¥¼ ê°€ì¥ ì˜ í¡ìˆ˜í•˜ëŠ”ì§€**\n",
    "* ì´ ë‹¨ê³„ ê²°ê³¼ê°€ **RoR í›„ë³´ ê²°ì • ê¸°ì¤€**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 3ë‹¨ê³„: Baseline + Residual + RoR (ìµœì¢…)\n",
    "\n",
    "(â€» 2ë‹¨ê³„ ìƒìœ„ 2ê°œ ì¡°í•©ë§Œ í™•ì¥)\n",
    "\n",
    "| ì‹¤í—˜ ID | Baseline | Residual | RoR      |\n",
    "| ----- | -------- | -------- | -------- |\n",
    "| BRR-1 | XGBoost  | LightGBM | LightGBM |\n",
    "| BRR-2 | XGBoost  | LightGBM | CatBoost |\n",
    "| BRR-3 | LightGBM | CatBoost | LightGBM |\n",
    "\n",
    "ëª©ì :\n",
    "\n",
    "* RoR ë‹¨ê³„ê°€ **ì‹¤ì œë¡œ ì„±ëŠ¥ ê°œì„ ì„ ì£¼ëŠ”ì§€ ê²€ì¦**\n",
    "* ê³¼ì í•© ì—¬ë¶€ í™•ì¸\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ì´ ì‹¤í—˜ ìˆ˜ ìš”ì•½\n",
    "\n",
    "| ë‹¨ê³„                        | ì‹¤í—˜ ìˆ˜    |\n",
    "| ------------------------- | ------- |\n",
    "| Baseline                  | 3       |\n",
    "| Baseline + Residual       | 4       |\n",
    "| Baseline + Residual + RoR | 3       |\n",
    "| **ì´í•©**                    | **10ê°œ** |\n",
    "\n",
    "ğŸ‘‰ **75ë³€ìˆ˜ Ã— 3ë‹¨ê³„ êµ¬ì¡° ì¹˜ê³  ë§¤ìš° ì ì€ ì‹¤í—˜ ìˆ˜**\n",
    "ğŸ‘‰ êµìˆ˜ë‹˜ ì…ì¥ì—ì„  â€œì‹¤í—˜ ì„¤ê³„ ì˜í–ˆë‹¤â€ëŠ” íŒë‹¨ ë‚˜ì˜´\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ ì‹¤í—˜ ìˆ˜ ìµœì†Œí™” ì „ëµ (ì´ê²Œ ì§„ì§œ ì¤‘ìš”)\n",
    "\n",
    "## ğŸ”¹ ì „ëµ 1: Stage-wise pruning (ë‹¨ê³„ë³„ íƒˆë½)\n",
    "\n",
    "âŒ ì˜ëª»ëœ ë°©ì‹\n",
    "\n",
    "* ëª¨ë“  ëª¨ë¸ Ã— ëª¨ë“  ë‹¨ê³„ Ã— ëª¨ë“  ì¡°í•©\n",
    "\n",
    "â­• ì˜¬ë°”ë¥¸ ë°©ì‹\n",
    "\n",
    "* **ê° Stageì—ì„œ ì„±ëŠ¥ í•˜ìœ„ ëª¨ë¸ ì¦‰ì‹œ íƒˆë½**\n",
    "\n",
    "ë…¼ë¦¬:\n",
    "\n",
    "> Baselineì—ì„œ êµ¬ì¡°ë¥¼ ëª» ì¡ëŠ” ëª¨ë¸ì€\n",
    "> Residualì´ë‚˜ RoRì—ì„œë„ ì˜ë¯¸ ìˆëŠ” ê°œì„ ì„ ë§Œë“¤ê¸° ì–´ë µë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ì „ëµ 2: ì—­í•  ê¸°ë°˜ ëª¨ë¸ ë°°ì¹˜\n",
    "\n",
    "| Stage    | ìš”êµ¬ ì„±ì§ˆ         | ì í•© ëª¨ë¸    |\n",
    "| -------- | ------------- | -------- |\n",
    "| Baseline | ê°•í•œ ë¹„ì„ í˜•, í° êµ¬ì¡°  | XGBoost  |\n",
    "| Residual | ì•ˆì •ì„±, ë…¸ì´ì¦ˆ í¡ìˆ˜   | LightGBM |\n",
    "| RoR      | ê³¼ì í•© ì–µì œ, ë¯¸ì„¸ ì¡°ì • | CatBoost |\n",
    "\n",
    "ğŸ‘‰ **ëª¨ë“  ëª¨ë¸ì„ ëª¨ë“  Stageì— ë„£ì„ í•„ìš” ì—†ìŒ**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ì „ëµ 3: RoRëŠ” â€œê²€ì¦ìš©â€ ë‹¨ê³„ë¡œ ì œí•œ\n",
    "\n",
    "* RoRëŠ”:\n",
    "\n",
    "  * í•­ìƒ ì„±ëŠ¥ì„ ì˜¬ë ¤ì£¼ì§€ ì•ŠìŒ\n",
    "  * ê³¼ì í•© ìœ„í—˜ ì¡´ì¬\n",
    "\n",
    "ê·¸ë˜ì„œ:\n",
    "\n",
    "* **Residual ë‹¨ê³„ì—ì„œ ëª…í™•í•œ ê°œì„ ì´ ìˆëŠ” ê²½ìš°ë§Œ RoR í™•ì¥**\n",
    "* RoR-only ì‹¤í—˜ì€ í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ì „ëµ 4: í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ê³ ì • or ìµœì†Œí™”\n",
    "\n",
    "* GridSearch âŒ\n",
    "* Bayesian âŒ\n",
    "* ê¸°ë³¸ê°’ + ì†Œí­ ì¡°ì •ë§Œ\n",
    "\n",
    "ì´ìœ :\n",
    "\n",
    "* ê³¼ì œ ëª©ì ì€ **êµ¬ì¡° ë¹„êµ**\n",
    "* íŠœë‹ ì‹¸ì›€ ì•„ë‹˜\n",
    "\n",
    "ë³´ê³ ì„œìš© ë¬¸ì¥:\n",
    "\n",
    "> ë³¸ ì—°êµ¬ì—ì„œëŠ” ëª¨ë¸ ê°„ êµ¬ì¡°ì  ë¹„êµì— ì´ˆì ì„ ë‘ê¸° ìœ„í•´\n",
    "> í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì€ ìµœì†Œí•œìœ¼ë¡œ ì œí•œí•˜ì˜€ë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ ì „ëµ 5: ë™ì¼ Validation / Test ê³ ì •\n",
    "\n",
    "* Validation: 2025-08-04 ~ 2025-10-20\n",
    "* Test: 2025-10-27 ~ 2026-01-12\n",
    "* **ëª¨ë“  ì‹¤í—˜ ë™ì¼ êµ¬ê°„**\n",
    "\n",
    "ğŸ‘‰ ì´ê²Œ ì—†ìœ¼ë©´ ì‹¤í—˜ ì˜ë¯¸ ì—†ìŒ\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ êµìˆ˜ë‹˜ì´ ì¢‹ì•„í•  â€œì„¤ê³„ ì² í•™â€ ë¬¸ì¥ (ì¤‘ìš”)\n",
    "\n",
    "ì´ ë¬¸ë‹¨ ê·¸ëŒ€ë¡œ ì¨ë„ ë¨:\n",
    "\n",
    "> ë³¸ ì—°êµ¬ì—ì„œëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ì¡°í•©ì„ íƒìƒ‰í•˜ëŠ” ëŒ€ì‹ ,\n",
    "> Baselineâ€“Residualâ€“RoRì˜ ë‹¨ê³„ì  êµ¬ì¡°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ\n",
    "> ê° ë‹¨ê³„ì—ì„œ ì—­í• ì— ë¶€í•©í•˜ëŠ” ëª¨ë¸ë§Œì„ ì„ íƒí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ\n",
    "> ì‹¤í—˜ ê³µê°„ì„ ì˜ë„ì ìœ¼ë¡œ ì¶•ì†Œí•˜ì˜€ë‹¤.\n",
    "> ì´ë¥¼ í†µí•´ ì œí•œëœ ì‹¤í—˜ ìˆ˜ ë‚´ì—ì„œ\n",
    "> êµ¬ì¡°ì  ì„±ëŠ¥ ê°œì„  ì—¬ë¶€ë¥¼ ëª…í™•íˆ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í•˜ì˜€ë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready\n"
     ]
    }
   ],
   "source": [
    "# 0. Environment Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "print(\"Environment Ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "ëª©í‘œ: ë‹ˆì¼ˆ ê°€ê²©(y)ì„ ì •ì˜í•˜ê³  ì‹œê°„ìˆœ ì •ë ¬, ê²°ì¸¡ ì²˜ë¦¬, ë³€í™˜ì„ ìˆ˜í–‰.\n",
    "í•´ì„: yëŠ” **ê°€ê²© ë ˆë²¨**ì´ë©°, xëŠ” ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ **1ì£¼ ì§€ì—°ëœ ìˆ˜ìµë¥ /ì°¨ë¶„**ì„ ì‚¬ìš©.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering ì„¤ëª… (CatBoost ì˜í–¥ ê´€ì )\n",
    "\n",
    "ì•„ë˜ ì „ì²˜ë¦¬ ì…€ì—ì„œ ìˆ˜í–‰í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íë¦„ì„ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. CatBoost ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” í¬ì¸íŠ¸ë„ í•¨ê»˜ í‘œì‹œí–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1) **ì»¬ëŸ¼ ì„ íƒ(í•„í„°ë§)**\n",
    "- íƒ€ê¹ƒ(`Com_LME_Ni_Cash`) + ê±°ì‹œ/ê¸ˆìœµ ì§€í‘œ(`Idx_`, `Bonds_`, `EX_`) + ì›ìì¬/ê¸ˆì†(`Com_LME` ë° ê¸ˆì† ì´ë¦„ í¬í•¨) ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ê´€ë ¨ì„±ì´ ë‚®ì€ ì»¬ëŸ¼ì„ ì œê±°í•´ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê³  ëª¨ë¸ì´ ìœ ì˜ë¯¸í•œ ë¶„í• ì„ ì°¾ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "2) **ê²°ì¸¡ ì²˜ë¦¬**\n",
    "- `ffill()`ë¡œ ê³¼ê±° ê°’ìœ¼ë¡œ ì±„ìš°ê³ , ì´í›„ `dropna()`ë¡œ ë‚¨ì€ ê²°ì¸¡ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "- **ë¯¸ë˜ê°’ìœ¼ë¡œ ì±„ìš°ëŠ” ëˆ„ìˆ˜(backfill)ë¥¼ ì˜ë„ì ìœ¼ë¡œ í”¼í•¨**.\n",
    "- CatBoostëŠ” ê²°ì¸¡ì„ ìì²´ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, `ffill`ì€ **ì¸ìœ„ì ì¸ ì—°ì†ì„±/ì¶”ì„¸**ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì–´ ë¶„ê¸° ê·œì¹™ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "3) **ìˆ˜ìµë¥  ë³€í™˜ (ë¡œê·¸/ì°¨ë¶„)**\n",
    "- **ì–‘ìˆ˜ë§Œ ì¡´ì¬í•˜ëŠ” ì‹œê³„ì—´**ì€ `log(í˜„ì¬/ì´ì „)`ìœ¼ë¡œ ë¡œê·¸ ìˆ˜ìµë¥ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- **0/ìŒìˆ˜ í¬í•¨** ì»¬ëŸ¼ì€ ë¡œê·¸ê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ë‹¨ìˆœ ì°¨ë¶„(`diff`)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ë¡œê·¸ ìˆ˜ìµë¥ ì€ ë¶„í¬ë¥¼ ë” ëŒ€ì¹­ì ìœ¼ë¡œ ë§Œë“¤ê³  ìŠ¤ì¼€ì¼ì„ ì••ì¶•í•´ **íŠ¸ë¦¬ ë¶„í• ì˜ ì•ˆì •ì„±**ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë°˜ë©´, ì°¨ë¶„ì€ ìŠ¤ì¼€ì¼ì´ í¬ê³  ë…¸ì´ì¦ˆê°€ ì»¤ì§ˆ ìˆ˜ ìˆì–´ **ë¶ˆì•ˆì •í•œ ë¶„ê¸°**ë¥¼ ìœ ë°œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "4) **íƒ€ê¹ƒ/í”¼ì²˜ ë¶„ë¦¬ì™€ ì‹œì  ì§€ì—°(ëˆ„ìˆ˜ ë°©ì§€)**\n",
    "- íƒ€ê¹ƒ `y`ëŠ” **ê°€ê²© ë ˆë²¨**(ì›ì‹œê°’)ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "- í”¼ì²˜ `X`ëŠ” **ìˆ˜ìµë¥ **ë¡œ ë§Œë“  ë’¤ `shift(1)` í•˜ì—¬ **1ì£¼ ì§€ì—°ëœ ì •ë³´ë§Œ ì‚¬ìš©**í•©ë‹ˆë‹¤.\n",
    "- ì¦‰, `X(t-1) â†’ y(t)` êµ¬ì¡°ë¡œ **ì •ë³´ ëˆ„ìˆ˜ ë°©ì§€**.\n",
    "- CatBoostëŠ” ê³¼ì í•©ì— ê°•í•˜ì§€ë§Œ, **ëˆ„ìˆ˜ ì œê±°ëŠ” ì„±ëŠ¥ì˜ ì‹ ë¢°ì„±**ì„ í¬ê²Œ ì¢Œìš°í•©ë‹ˆë‹¤.\n",
    "\n",
    "5) **ì¸ë±ìŠ¤ ì •ë ¬**\n",
    "- `shift`/`diff`ë¡œ ìƒê¸´ NaNì„ ì œê±°í•˜ê³ , `X`/`y`ì˜ **ê³µí†µ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©**í•˜ì—¬ ì™„ì „ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "- í•™ìŠµ ìƒ˜í”Œ ìˆ˜ê°€ ì¤„ì–´ë“¤ì§€ë§Œ, **ì •í™•í•œ ì‹œì  ëŒ€ì‘**ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**CatBoost ì„±ëŠ¥ì— ì˜í–¥ ê°€ëŠ¥ì„± ìš”ì•½**\n",
    "- íŠ¸ë¦¬ ëª¨ë¸ì€ ìŠ¤ì¼€ì¼ë§ì— ë‘”ê°í•˜ì§€ë§Œ **ë¶„í¬ í˜•íƒœ(ë¡œê·¸/ì°¨ë¶„)**ì™€ **ë…¸ì´ì¦ˆ ìˆ˜ì¤€**ì— ë¯¼ê°í•©ë‹ˆë‹¤.\n",
    "- ìˆ˜ìµë¥  ê¸°ë°˜ í”¼ì²˜ëŠ” **ë‹¨ê¸° ë³€í™”**ë¥¼ ê°•ì¡°í•˜ê³  **ë ˆë²¨ ì •ë³´**ë¥¼ í¬ì„ì‹œì¼œ, CatBoostê°€ í° ì¶”ì„¸ë¥¼ ì¡ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- `ffill`ë¡œ ìƒì„±ëœ êµ¬ê°„ì€ ë³€ë™ì„±ì´ ë‚®ì•„ì ¸ **ê³¼ë„í•œ ë³´ìˆ˜ì  ë¶„ê¸°**ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•„ìš”í•˜ë©´ ë‹¤ìŒì„ ì¶”ê°€ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ëˆ„ìˆ˜ ì£¼ì˜):\n",
    "- íƒ€ê¹ƒ ë ˆë²¨ì˜ **lag(1~k)** ë¥¼ í”¼ì²˜ë¡œ ì¶”ê°€\n",
    "- ìˆ˜ìµë¥  ì™¸ì— **ì´ë™í‰ê· /ë³€ë™ì„±(rolling std)** ë“± ë ˆë²¨ ê¸°ë°˜ íŒŒìƒ í”¼ì²˜\n",
    "- ë¹„ì •ìƒì„± ì™„í™” ëª©ì ì˜ **Box-Cox/Robust ë³€í™˜** ë¹„êµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-positive columns transformed with diff: ['Bonds_US_3M']\n",
      "Feature Count: 51\n",
      "Total Samples (aligned): 666\n",
      "Data leakage prevented: Using X(t-1) to predict y(t)\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Filtering & Preprocessing\n",
    "filename = \"data_weekly_260120.csv\"\n",
    "df_raw = pd.read_csv(filename)\n",
    "df_raw[\"dt\"] = pd.to_datetime(df_raw[\"dt\"])\n",
    "df_raw = df_raw.set_index(\"dt\").sort_index()\n",
    "\n",
    "def filter_cols(columns):\n",
    "    target = \"Com_LME_Ni_Cash\"\n",
    "    metals = [\"Gold\", \"Silver\", \"Iron\", \"Steel\", \"Copper\", \"Aluminum\", \"Zinc\", \"Nickel\", \"Lead\", \"Tin\", \"Uranium\"]\n",
    "    filtered = [target]\n",
    "    for col in columns:\n",
    "        if any(x in col for x in [\"Idx_\", \"Bonds_\", \"EX_\"]):\n",
    "            filtered.append(col)\n",
    "        elif \"Com_LME\" in col:\n",
    "            filtered.append(col)\n",
    "        elif any(m in col for m in metals):\n",
    "            filtered.append(col)\n",
    "    return sorted(list(set(filtered)))\n",
    "\n",
    "filtered_cols = filter_cols(df_raw.columns)\n",
    "df = df_raw[filtered_cols].copy()\n",
    "df = df.ffill()\n",
    "df = df.dropna()  # avoid backfilling from future data\n",
    "\n",
    "# Returns for features: log for positive-only series, diff for non-positive series\n",
    "non_pos_cols = [c for c in df.columns if (df[c] <= 0).any()]\n",
    "pos_cols = [c for c in df.columns if c not in non_pos_cols]\n",
    "\n",
    "df_ret = pd.DataFrame(index=df.index)\n",
    "df_ret[pos_cols] = np.log(df[pos_cols] / df[pos_cols].shift(1))\n",
    "df_ret[non_pos_cols] = df[non_pos_cols].diff()\n",
    "\n",
    "if non_pos_cols:\n",
    "    print(f\"Non-positive columns transformed with diff: {non_pos_cols}\")\n",
    "\n",
    "# --- NO LEAKAGE: Lag features by 1 week ---\n",
    "target_col = \"Com_LME_Ni_Cash\"\n",
    "y = df[target_col]  # price level target\n",
    "X = df_ret.drop(columns=[target_col]).shift(1)  # Lagged feature returns\n",
    "\n",
    "# Align y and X by dropping NaNs introduced by shift and diff\n",
    "valid_idx = X.dropna().index.intersection(y.dropna().index)\n",
    "X = X.loc[valid_idx]\n",
    "y = y.loc[valid_idx]\n",
    "\n",
    "print(f\"Feature Count: {X.shape[1]}\")\n",
    "print(f\"Total Samples (aligned): {len(y)}\")\n",
    "print(\"Data leakage prevented: Using X(t-1) to predict y(t)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Overview\n",
    "ëª©í‘œ: ê°€ê²© ì¶”ì„¸, ìˆ˜ìµë¥  ë¶„í¬, ë³€ë™ì„± ìˆ˜ì¤€ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸.\n",
    "í•´ì„: ì´ìƒì¹˜ë‚˜ ë³€ë™ì„± ê¸‰ë“±ì€ ëª¨ë¸ ë¶ˆì•ˆì • ì‹ í˜¸.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Data Overview Plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(df.index, df[target_col], color='black', linewidth=1)\n",
    "plt.title('Nickel Price (Level)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df_ret[target_col].dropna(), bins=30, color='steelblue')\n",
    "plt.title('Nickel Return Distribution')\n",
    "plt.show()\n",
    "\n",
    "rolling_vol = df_ret[target_col].rolling(12).std() * np.sqrt(52)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(rolling_vol.index, rolling_vol, color='darkorange', linewidth=1)\n",
    "plt.title('Nickel Rolling Volatility (12w, annualized)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks\n",
    "ëª©í‘œ: ì£¼ê°„ ì£¼ê¸°(7ì¼)ì™€ ê²°ì¸¡ ì—¬ë¶€ë¥¼ í™•ì¸.\n",
    "í•´ì„: ì£¼ê¸°ê°€ ê¹¨ì§€ë©´ ë¦¬ìƒ˜í”Œë§ í•„ìš”, ê²°ì¸¡ì€ ì¬ì²˜ë¦¬ í•„ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is clean. No missing values found.\n",
      "\n",
      "Time Frequency Distribution (Days):\n",
      "dt\n",
      "7.0    667\n",
      "Name: count, dtype: int64\n",
      "Data confirmed as Weekly.\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Quality Check\n",
    "missing = df.isnull().sum().sum()\n",
    "if missing > 0:\n",
    "    print(f\"Missing values remain after forward fill: {missing}\")\n",
    "    print(\"Consider dropping initial rows or imputing within train only.\")\n",
    "else:\n",
    "    print(\"Data is clean. No missing values found.\")\n",
    "\n",
    "# Frequency Check\n",
    "freq_check = df.index.to_series().diff().dt.days.value_counts()\n",
    "print(\"\\nTime Frequency Distribution (Days):\")\n",
    "print(freq_check)\n",
    "\n",
    "if freq_check.index[0] != 7:\n",
    "    print(\"WARNING: Data is not strictly weekly. Consider resampling if necessary.\")\n",
    "else:\n",
    "    print(\"Data confirmed as Weekly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection (Train-Only)\n",
    "ëª©í‘œ: SHAP ê¸°ë°˜ìœ¼ë¡œ ë‹ˆì¼ˆì— ì˜í–¥ í° ë³€ìˆ˜ë¥¼ ì„ íƒ.\n",
    "í•´ì„: train-onlyë¡œ ì„ íƒí•´ validation/test ì •ë³´ ëˆ„ìˆ˜ë¥¼ ì°¨ë‹¨.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2013-04-15 ~ 2025-07-28 (642)\n",
      "Val  : 2025-08-04 ~ 2025-10-20 (12)\n",
      "Test : 2025-10-27 ~ 2026-01-12 (12)\n",
      "Selected Top 20: ['Com_LME_Ni_Inv', 'Bonds_US_3M', 'Bonds_AUS_1Y', 'Com_LME_Al_Inv', 'Bonds_BRZ_1Y'] ...\n",
      "Train samples: 642, Val samples: 12, Test samples: 12\n"
     ]
    }
   ],
   "source": [
    "# 3. Train/Validation/Test Split (fixed date ranges) + SHAP Feature Selection (train-only)\n",
    "VAL_START = \"2025-08-04\"\n",
    "VAL_END = \"2025-10-20\"\n",
    "TEST_START = \"2025-10-27\"\n",
    "TEST_END = \"2026-01-12\"\n",
    "\n",
    "# Split by fixed dates\n",
    "X_train_all = X.loc[X.index < VAL_START]\n",
    "y_train = y.loc[X_train_all.index]\n",
    "\n",
    "X_val_all = X.loc[(X.index >= VAL_START) & (X.index <= VAL_END)]\n",
    "y_val = y.loc[X_val_all.index]\n",
    "\n",
    "X_test_all = X.loc[(X.index >= TEST_START) & (X.index <= TEST_END)]\n",
    "y_test = y.loc[X_test_all.index]\n",
    "\n",
    "# Sanity checks\n",
    "if len(X_train_all) == 0 or len(X_val_all) == 0 or len(X_test_all) == 0:\n",
    "    raise ValueError(\"Split resulted in empty train/val/test. Check date ranges or data coverage.\")\n",
    "\n",
    "print(f\"Train: {X_train_all.index.min().date()} ~ {X_train_all.index.max().date()} ({len(X_train_all)})\")\n",
    "print(f\"Val  : {X_val_all.index.min().date()} ~ {X_val_all.index.max().date()} ({len(X_val_all)})\")\n",
    "print(f\"Test : {X_test_all.index.min().date()} ~ {X_test_all.index.max().date()} ({len(X_test_all)})\")\n",
    "\n",
    "# SHAP feature selection (train-only)\n",
    "model_shap = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_shap.fit(X_train_all, y_train)\n",
    "explainer = shap.TreeExplainer(model_shap)\n",
    "shap_val = explainer.shap_values(X_train_all)\n",
    "importances = np.abs(shap_val).mean(axis=0)\n",
    "feat_imp = pd.DataFrame({\"feature\": X_train_all.columns, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "selected_features = feat_imp.head(top_n)[\"feature\"].tolist()\n",
    "\n",
    "# Use the train-selected features for all splits\n",
    "X_train = X_train_all[selected_features]\n",
    "X_val = X_val_all[selected_features]\n",
    "X_test = X_test_all[selected_features]\n",
    "\n",
    "print(f\"Selected Top {top_n}: {selected_features[:5]} ...\")\n",
    "print(f\"Train samples: {len(X_train)}, Val samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature Importance & Correlation\n",
    "ëª©í‘œ: ì„ íƒëœ í”¼ì²˜ì˜ ì¤‘ìš”ë„ì™€ ìƒê´€ êµ¬ì¡° ì ê²€.\n",
    "í•´ì„: ë†’ì€ ìƒê´€ì€ ì •ë³´ ì¤‘ë³µ ê°€ëŠ¥ì„±.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Com_LME_Ni_Inv</td>\n",
       "      <td>793.194336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Bonds_US_3M</td>\n",
       "      <td>781.705994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bonds_AUS_1Y</td>\n",
       "      <td>709.356140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Com_LME_Al_Inv</td>\n",
       "      <td>432.853821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bonds_BRZ_1Y</td>\n",
       "      <td>393.670135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bonds_IND_1Y</td>\n",
       "      <td>343.780731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Com_Steel</td>\n",
       "      <td>255.708038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Com_LME_Zn_Inv</td>\n",
       "      <td>243.577835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Com_LME_Sn_Cash</td>\n",
       "      <td>239.477875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Com_HRC_Steel</td>\n",
       "      <td>205.358200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  importance\n",
       "25   Com_LME_Ni_Inv  793.194336\n",
       "50      Bonds_US_3M  781.705994\n",
       "1      Bonds_AUS_1Y  709.356140\n",
       "21   Com_LME_Al_Inv  432.853821\n",
       "3      Bonds_BRZ_1Y  393.670135\n",
       "11     Bonds_IND_1Y  343.780731\n",
       "33        Com_Steel  255.708038\n",
       "31   Com_LME_Zn_Inv  243.577835\n",
       "28  Com_LME_Sn_Cash  239.477875\n",
       "18    Com_HRC_Steel  205.358200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3.1 Feature Importance & Correlation\n",
    "top_feat = feat_imp.head(top_n).sort_values('importance')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(top_feat['feature'], top_feat['importance'], color='teal')\n",
    "plt.title('Top Feature Importance (SHAP, Train Only)')\n",
    "plt.show()\n",
    "\n",
    "corr = X_train_all[selected_features].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap (Selected Features, Train)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Stability (SHAP across folds)\n",
    "ëª©í‘œ: ì„ íƒëœ í”¼ì²˜ê°€ foldë§ˆë‹¤ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸.\n",
    "í•´ì„: ì•ˆì •ì„±ì´ ë‚®ìœ¼ë©´ í”¼ì²˜ ì„ íƒì´ ë°ì´í„° êµ¬ê°„ì— ë¯¼ê°í•˜ë‹¤ëŠ” ì˜ë¯¸.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature stability (top):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>count</th>\n",
       "      <th>stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Com_LME_Pb_Inv</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Com_LME_Cu_Inv</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Com_Steel</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Com_LME_Ni_Inv</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bonds_BRZ_1Y</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bonds_AUS_1Y</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bonds_CHN_1Y</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bonds_IND_1Y</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Com_LME_Zn_Inv</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Com_HRC_Steel</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  count  stability\n",
       "0   Com_LME_Pb_Inv      5        1.0\n",
       "9   Com_LME_Cu_Inv      5        1.0\n",
       "19       Com_Steel      5        1.0\n",
       "17  Com_LME_Ni_Inv      5        1.0\n",
       "15    Bonds_BRZ_1Y      5        1.0\n",
       "1     Bonds_AUS_1Y      5        1.0\n",
       "13    Bonds_CHN_1Y      5        1.0\n",
       "10    Bonds_IND_1Y      5        1.0\n",
       "14  Com_LME_Zn_Inv      5        1.0\n",
       "8    Com_HRC_Steel      5        1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Jaccard similarity: 0.714\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Feature Stability (SHAP across folds)\n",
    "from collections import Counter\n",
    "\n",
    "fs_val_horizon = 12\n",
    "fs_n_samples = len(X_train_all)\n",
    "fs_max_splits = max(1, (fs_n_samples // fs_val_horizon) - 1)\n",
    "fs_n_splits = min(5, fs_max_splits)\n",
    "\n",
    "fs_splits = []\n",
    "for i in range(fs_n_splits):\n",
    "    val_end = fs_n_samples - (fs_n_splits - 1 - i) * fs_val_horizon\n",
    "    val_start = val_end - fs_val_horizon\n",
    "    train_end = val_start\n",
    "    fs_splits.append((slice(0, train_end), slice(val_start, val_end)))\n",
    "\n",
    "feature_sets = []\n",
    "for tr_slice, _ in fs_splits:\n",
    "    X_fs = X_train_all.iloc[tr_slice]\n",
    "    y_fs = y_train.iloc[tr_slice]\n",
    "    model_fs = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model_fs.fit(X_fs, y_fs)\n",
    "    explainer_fs = shap.TreeExplainer(model_fs)\n",
    "    shap_val_fs = explainer_fs.shap_values(X_fs)\n",
    "    importances_fs = np.abs(shap_val_fs).mean(axis=0)\n",
    "    feat_imp_fs = pd.DataFrame({\"feature\": X_fs.columns, \"importance\": importances_fs}).sort_values(\"importance\", ascending=False)\n",
    "    top_feats = feat_imp_fs.head(top_n)[\"feature\"].tolist()\n",
    "    feature_sets.append(set(top_feats))\n",
    "\n",
    "counter = Counter([f for s in feature_sets for f in s])\n",
    "stability_df = pd.DataFrame({\"feature\": list(counter.keys()), \"count\": list(counter.values())})\n",
    "stability_df[\"stability\"] = stability_df[\"count\"] / fs_n_splits\n",
    "stability_df = stability_df.sort_values(\"stability\", ascending=False)\n",
    "print(\"Feature stability (top 20):\")\n",
    "display(stability_df.head(20))\n",
    "\n",
    "avg_jaccard = np.nan\n",
    "if len(feature_sets) > 1:\n",
    "    sims = []\n",
    "    for i in range(len(feature_sets)):\n",
    "        for j in range(i + 1, len(feature_sets)):\n",
    "            a, b = feature_sets[i], feature_sets[j]\n",
    "            sims.append(len(a & b) / len(a | b) if (a | b) else np.nan)\n",
    "    avg_jaccard = float(np.nanmean(sims))\n",
    "    print(f\"Avg Jaccard similarity: {avg_jaccard:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough splits for stability analysis.\")\n",
    "\n",
    "feature_stability_top = stability_df.head(20)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(feature_stability_top.sort_values(\"stability\")[\"feature\"], feature_stability_top.sort_values(\"stability\")[\"stability\"], color=\"slateblue\")\n",
    "plt.title(\"Feature Stability Across Walk-Forward Splits\")\n",
    "plt.xlabel(\"Stability (Frequency)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training & Evaluation\n",
    "ëª©í‘œ: Baseline -> Residual -> ROR ìŠ¤íƒì„ **ëª¨ë“  ëª¨ë¸ ì¡°í•©**ìœ¼ë¡œ ë¹„êµ.\n",
    "í•´ì„: validation ê¸°ì¤€ìœ¼ë¡œ ì¡°í•©ì„ ì„ íƒí•˜ê³ , testì—ì„œ ìµœì¢… í‰ê°€.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline Models\n",
    "ëª©í‘œ: ê° ëª¨ë¸ ë‹¨ë… ì˜ˆì¸¡(=baseline) ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •.\n",
    "í•´ì„: ì´ë³´ë‹¤ ë‚˜ì€ ì¡°í•©ë§Œ ì±„íƒ ê°€ì¹˜.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 Modeling Utilities\n",
    "import math\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    eps = 1e-8\n",
    "    denom = np.where(np.abs(y_true) < eps, np.nan, np.abs(y_true))\n",
    "    ape = np.abs((y_true - y_pred) / denom)\n",
    "    spe = ((y_true - y_pred) / denom) ** 2\n",
    "    mape = np.nanmean(ape) * 100\n",
    "    rmspe = np.sqrt(np.nanmean(spe)) * 100\n",
    "\n",
    "    return rmse, rmspe, mape, mae\n",
    "\n",
    "\n",
    "def build_model(name, params=None):\n",
    "    params = params or {}\n",
    "    if name == 'XGBoost':\n",
    "        base = dict(n_estimators=500, learning_rate=0.05, random_state=42, n_jobs=-1)\n",
    "        base.update(params)\n",
    "        return xgb.XGBRegressor(**base)\n",
    "    if name == 'LightGBM':\n",
    "        base = dict(n_estimators=500, learning_rate=0.05, random_state=42, n_jobs=-1, verbose=-1, subsample_freq=1)\n",
    "        base.update(params)\n",
    "        return lgb.LGBMRegressor(**base)\n",
    "    if name == 'CatBoost':\n",
    "        base = dict(n_estimators=500, learning_rate=0.05, random_state=42, verbose=0, allow_writing_files=False)\n",
    "        base.update(params)\n",
    "        return CatBoostRegressor(**base)\n",
    "    if name == 'GradientBoosting':\n",
    "        base = dict(n_estimators=500, learning_rate=0.05, random_state=42)\n",
    "        base.update(params)\n",
    "        return GradientBoostingRegressor(**base)\n",
    "    if name == 'AdaBoost':\n",
    "        base = dict(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "        base.update(params)\n",
    "        return AdaBoostRegressor(**base)\n",
    "    raise ValueError(f\"Unknown model: {name}\")\n",
    "\n",
    "\n",
    "def compute_prev_prices(y, idx):\n",
    "    idx = pd.Index(idx)\n",
    "    if len(idx) == 0:\n",
    "        return y.shift(1).reindex(idx).copy()\n",
    "    prev = y.shift(1).reindex(idx).copy()\n",
    "    # If the split starts at the first available point, fall back to current price\n",
    "    if pd.isna(prev.iloc[0]):\n",
    "        prior = y.loc[y.index < idx[0]]\n",
    "        if len(prior) > 0:\n",
    "            prev.iloc[0] = prior.iloc[-1]\n",
    "        else:\n",
    "            prev.iloc[0] = y.loc[idx[0]]\n",
    "    return prev\n",
    "\n",
    "\n",
    "def make_meta_features(X, base_pred, resid_pred):\n",
    "    X_meta = X.copy()\n",
    "    X_meta['base_pred'] = base_pred\n",
    "    X_meta['resid_pred'] = resid_pred\n",
    "    return X_meta\n",
    "\n",
    "\n",
    "def ror_target(y_true, prev_prices, base_pred, resid_pred, mode='return'):\n",
    "    if mode == 'return':\n",
    "        return (y_true / prev_prices) - 1\n",
    "    if mode == 'pred_minus_residual':\n",
    "        # ror = baseline_pred - (actual - baseline_pred) = 2*baseline_pred - actual\n",
    "        return (2 * base_pred) - y_true\n",
    "    raise ValueError(f\"Unknown ROR mode: {mode}\")\n",
    "\n",
    "\n",
    "def ror_pred_to_price(ror_pred, prev_prices, base_pred, resid_pred, mode='return'):\n",
    "    if mode == 'return':\n",
    "        return prev_prices * (1 + ror_pred)\n",
    "    if mode == 'pred_minus_residual':\n",
    "        return (2 * base_pred) - ror_pred\n",
    "    raise ValueError(f\"Unknown ROR mode: {mode}\")\n",
    "\n",
    "\n",
    "# Hyperparameter tuning setup (randomized search over wider spaces)\n",
    "TUNING_BUDGET = {\n",
    "    'XGBoost': 16,\n",
    "    'LightGBM': 16,\n",
    "    'CatBoost': 12,\n",
    "    'GradientBoosting': 10,\n",
    "    'AdaBoost': 10\n",
    "}\n",
    "\n",
    "param_spaces = {\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [300, 600, 900, 1200],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'min_child_weight': [1, 2, 5, 10],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.3],\n",
    "        'reg_alpha': [0, 1e-3, 1e-2, 1e-1],\n",
    "        'reg_lambda': [0.5, 1, 2, 5]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [300, 600, 900, 1200],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "        'num_leaves': [31, 63, 127, 255],\n",
    "        'max_depth': [-1, 4, 6, 8],\n",
    "        'min_child_samples': [10, 20, 30, 50],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 1e-3, 1e-2, 1e-1],\n",
    "        'reg_lambda': [0, 0.5, 1, 2]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'n_estimators': [300, 600, 1000],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "        'depth': [4, 6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "        'random_strength': [0.0, 0.5, 1.0, 2.0],\n",
    "        'bagging_temperature': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [200, 400, 800],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [100, 200, 400, 600],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'loss': ['linear', 'square', 'exponential']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def sample_param_grid(space, n_iter, seed):\n",
    "    sampler = ParameterSampler(space, n_iter=n_iter, random_state=seed)\n",
    "    return list(sampler)\n",
    "\n",
    "\n",
    "param_grids = {}\n",
    "for i, name in enumerate(param_spaces.keys()):\n",
    "    param_grids[name] = sample_param_grid(param_spaces[name], TUNING_BUDGET[name], RANDOM_SEED + i)\n",
    "\n",
    "# Model pools (no pruning)\n",
    "ALL_MODELS = list(param_grids.keys())\n",
    "BASELINE_MODELS = ALL_MODELS\n",
    "RESIDUAL_MODELS = ALL_MODELS\n",
    "ROR_MODELS = ALL_MODELS\n",
    "\n",
    "# ROR mode: 'return' or 'pred_minus_residual' (price-level via 2*base_pred - ror_pred)\n",
    "ROR_TARGET_MODE = 'pred_minus_residual'\n",
    "\n",
    "# Stage-wise pruning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results (Test):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSPE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive_Last</th>\n",
       "      <td>569.225486</td>\n",
       "      <td>3.498880</td>\n",
       "      <td>2.577581</td>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Drift</th>\n",
       "      <td>480.665760</td>\n",
       "      <td>3.070722</td>\n",
       "      <td>2.103092</td>\n",
       "      <td>325.763889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   RMSE     RMSPE      MAPE         MAE\n",
       "Model                                                  \n",
       "Naive_Last   569.225486  3.498880  2.577581  410.000000\n",
       "Naive_Drift  480.665760  3.070722  2.103092  325.763889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.1 Baseline Models\n",
    "preds = pd.DataFrame(index=y_test.index)\n",
    "preds['Actual'] = y_test\n",
    "results = []\n",
    "\n",
    "prev_price = y.shift(1).loc[y_test.index].copy()\n",
    "prev_prev_price = y.shift(2).loc[y_test.index].copy()\n",
    "\n",
    "last_before_test = y.loc[y.index < y_test.index[0]]\n",
    "if len(last_before_test) < 2:\n",
    "    raise ValueError(\"Not enough history before test to build baselines.\")\n",
    "\n",
    "# Fill first test point(s) with last available prices (typically from validation)\n",
    "prev_price.iloc[0] = last_before_test.iloc[-1]\n",
    "prev_prev_price.iloc[0] = last_before_test.iloc[-2]\n",
    "\n",
    "naive_last = prev_price\n",
    "naive_drift = prev_price + (prev_price - prev_prev_price)\n",
    "\n",
    "baseline_models = {\n",
    "    'Naive_Last': naive_last,\n",
    "    'Naive_Drift': naive_drift\n",
    "}\n",
    "\n",
    "baseline_rows = []\n",
    "for name, y_pred in baseline_models.items():\n",
    "    preds[name] = y_pred\n",
    "    rmse, rmspe, mape, mae = eval_metrics(y_test, y_pred)\n",
    "    row = {\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'RMSPE': rmspe,\n",
    "        'MAPE': mape,\n",
    "        'MAE': mae\n",
    "    }\n",
    "    results.append(row)\n",
    "    baseline_rows.append(row)\n",
    "\n",
    "baseline_results_df = pd.DataFrame(baseline_rows).set_index('Model')\n",
    "print(\"Baseline Results (Test):\")\n",
    "display(baseline_results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Baseline/Residual/ROR Stacking Search\n",
    "ëª©í‘œ: Baseline -> Residual -> ROR ì¡°í•©ì„ validationìœ¼ë¡œ ë¹„êµ.\n",
    "í•´ì„: ì¡°í•© ì„ íƒì€ validation ê¸°ì¤€, testëŠ” ìµœì¢… í‰ê°€.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline validation (best params per model):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>VAL_RMSE</th>\n",
       "      <th>VAL_RMSPE</th>\n",
       "      <th>VAL_MAPE</th>\n",
       "      <th>VAL_MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>395.833323</td>\n",
       "      <td>2.626477</td>\n",
       "      <td>2.270474</td>\n",
       "      <td>341.885212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>433.009935</td>\n",
       "      <td>2.878858</td>\n",
       "      <td>2.230754</td>\n",
       "      <td>335.340897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>{'n_estimators': 400, 'loss': 'linear', 'learn...</td>\n",
       "      <td>511.368371</td>\n",
       "      <td>3.392556</td>\n",
       "      <td>3.113115</td>\n",
       "      <td>468.989584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 0.5, 'reg_alp...</td>\n",
       "      <td>763.832533</td>\n",
       "      <td>5.096360</td>\n",
       "      <td>4.233414</td>\n",
       "      <td>635.111193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>{'subsample': 0.8, 'reg_lambda': 0.5, 'reg_alp...</td>\n",
       "      <td>766.313199</td>\n",
       "      <td>5.105188</td>\n",
       "      <td>4.358079</td>\n",
       "      <td>654.739746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Params  \\\n",
       "Model                                                                 \n",
       "GradientBoosting  {'subsample': 0.8, 'n_estimators': 200, 'min_s...   \n",
       "CatBoost          {'random_strength': 2.0, 'n_estimators': 300, ...   \n",
       "AdaBoost          {'n_estimators': 400, 'loss': 'linear', 'learn...   \n",
       "LightGBM          {'subsample': 0.6, 'reg_lambda': 0.5, 'reg_alp...   \n",
       "XGBoost           {'subsample': 0.8, 'reg_lambda': 0.5, 'reg_alp...   \n",
       "\n",
       "                    VAL_RMSE  VAL_RMSPE  VAL_MAPE     VAL_MAE  \n",
       "Model                                                          \n",
       "GradientBoosting  395.833323   2.626477  2.270474  341.885212  \n",
       "CatBoost          433.009935   2.878858  2.230754  335.340897  \n",
       "AdaBoost          511.368371   3.392556  3.113115  468.989584  \n",
       "LightGBM          763.832533   5.096360  4.233414  635.111193  \n",
       "XGBoost           766.313199   5.105188  4.358079  654.739746  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best baseline (val): GradientBoosting\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Baseline Model Selection (train -> val)\n",
    "baseline_search_rows = []\n",
    "best_params_by_model = {}\n",
    "base_pred_train = {}\n",
    "base_pred_val = {}\n",
    "\n",
    "for model_name in BASELINE_MODELS:\n",
    "    best_rmse = math.inf\n",
    "    best_params = None\n",
    "    for params in param_grids[model_name]:\n",
    "        model = build_model(model_name, params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        rmse, rmspe, mape, mae = eval_metrics(y_val, y_pred_val)\n",
    "        baseline_search_rows.append({\n",
    "            'Model': model_name,\n",
    "            'Params': params,\n",
    "            'VAL_RMSE': rmse,\n",
    "            'VAL_RMSPE': rmspe,\n",
    "            'VAL_MAPE': mape,\n",
    "            'VAL_MAE': mae\n",
    "        })\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_params = params\n",
    "\n",
    "    best_params_by_model[model_name] = best_params\n",
    "\n",
    "    # Fit best baseline model to get train/val preds (for residual/ROR stacking)\n",
    "    model = build_model(model_name, best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    base_pred_train[model_name] = model.predict(X_train)\n",
    "    base_pred_val[model_name] = model.predict(X_val)\n",
    "\n",
    "baseline_search_df = pd.DataFrame(baseline_search_rows)\n",
    "baseline_val_df = baseline_search_df.sort_values('VAL_RMSE').groupby('Model').head(1).set_index('Model')\n",
    "print(\"Baseline validation (best params per model):\")\n",
    "display(baseline_val_df)\n",
    "\n",
    "# Baseline ranking (all models)\n",
    "baseline_rank_df = baseline_val_df.sort_values('VAL_RMSE')\n",
    "best_baseline_model = baseline_rank_df.index[0] if len(baseline_rank_df) > 0 else None\n",
    "print(f\"Best baseline (val): {best_baseline_model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Validation Summary\n",
    "ëª©í‘œ: baseline ë° ì¡°í•© ì„±ëŠ¥ ìš”ì•½.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 Validation Summary Plots\n",
    "val_order = baseline_val_df.sort_values('VAL_RMSE').index.tolist()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=baseline_val_df.loc[val_order].index, y=baseline_val_df.loc[val_order]['VAL_RMSE'], color='steelblue')\n",
    "plt.title(\"Baseline Validation RMSE (Best Config per Model)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Residual + ROR Combo Search\n",
    "ëª©í‘œ: baseline + residual + ror ì¡°í•©ë³„ ì„±ëŠ¥ ë¹„êµ.\n",
    "í•´ì„: ì¢‹ì€ ì¡°í•©ì„ ì°¾ëŠ” ê²ƒì´ í•µì‹¬.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Residual combos (val):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>Residual</th>\n",
       "      <th>Residual_Params</th>\n",
       "      <th>VAL_RMSE</th>\n",
       "      <th>VAL_RMSPE</th>\n",
       "      <th>VAL_MAPE</th>\n",
       "      <th>VAL_MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>{'n_estimators': 100, 'loss': 'square', 'learn...</td>\n",
       "      <td>501.026243</td>\n",
       "      <td>3.332203</td>\n",
       "      <td>2.613648</td>\n",
       "      <td>393.013182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>{'n_estimators': 400, 'loss': 'square', 'learn...</td>\n",
       "      <td>512.752511</td>\n",
       "      <td>3.406364</td>\n",
       "      <td>2.813295</td>\n",
       "      <td>422.968138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>515.559496</td>\n",
       "      <td>3.431042</td>\n",
       "      <td>2.697754</td>\n",
       "      <td>405.476142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>{'n_estimators': 200, 'loss': 'exponential', '...</td>\n",
       "      <td>529.023400</td>\n",
       "      <td>3.526671</td>\n",
       "      <td>2.555540</td>\n",
       "      <td>383.877985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>552.695146</td>\n",
       "      <td>3.677013</td>\n",
       "      <td>2.819113</td>\n",
       "      <td>423.741072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>567.835114</td>\n",
       "      <td>3.780663</td>\n",
       "      <td>3.186102</td>\n",
       "      <td>478.613032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>579.285790</td>\n",
       "      <td>3.847126</td>\n",
       "      <td>2.996934</td>\n",
       "      <td>451.257815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>609.451808</td>\n",
       "      <td>4.049340</td>\n",
       "      <td>3.273997</td>\n",
       "      <td>492.265194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>626.800111</td>\n",
       "      <td>4.178778</td>\n",
       "      <td>3.219987</td>\n",
       "      <td>483.869164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'subsample': 0.6, 'reg_lambda': 0.5, 'reg_alp...</td>\n",
       "      <td>771.407001</td>\n",
       "      <td>5.143204</td>\n",
       "      <td>4.194746</td>\n",
       "      <td>629.493178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Base          Residual  \\\n",
       "185          AdaBoost          AdaBoost   \n",
       "119          CatBoost          AdaBoost   \n",
       "111          CatBoost  GradientBoosting   \n",
       "61   GradientBoosting          AdaBoost   \n",
       "47   GradientBoosting  GradientBoosting   \n",
       "41   GradientBoosting          CatBoost   \n",
       "175          AdaBoost  GradientBoosting   \n",
       "169          AdaBoost          CatBoost   \n",
       "105          CatBoost          CatBoost   \n",
       "148          AdaBoost          LightGBM   \n",
       "\n",
       "                                       Residual_Params    VAL_RMSE  VAL_RMSPE  \\\n",
       "185  {'n_estimators': 100, 'loss': 'square', 'learn...  501.026243   3.332203   \n",
       "119  {'n_estimators': 400, 'loss': 'square', 'learn...  512.752511   3.406364   \n",
       "111  {'subsample': 0.8, 'n_estimators': 200, 'min_s...  515.559496   3.431042   \n",
       "61   {'n_estimators': 200, 'loss': 'exponential', '...  529.023400   3.526671   \n",
       "47   {'subsample': 0.8, 'n_estimators': 200, 'min_s...  552.695146   3.677013   \n",
       "41   {'random_strength': 2.0, 'n_estimators': 300, ...  567.835114   3.780663   \n",
       "175  {'subsample': 0.8, 'n_estimators': 200, 'min_s...  579.285790   3.847126   \n",
       "169  {'random_strength': 2.0, 'n_estimators': 300, ...  609.451808   4.049340   \n",
       "105  {'random_strength': 2.0, 'n_estimators': 300, ...  626.800111   4.178778   \n",
       "148  {'subsample': 0.6, 'reg_lambda': 0.5, 'reg_alp...  771.407001   5.143204   \n",
       "\n",
       "     VAL_MAPE     VAL_MAE  \n",
       "185  2.613648  393.013182  \n",
       "119  2.813295  422.968138  \n",
       "111  2.697754  405.476142  \n",
       "61   2.555540  383.877985  \n",
       "47   2.819113  423.741072  \n",
       "41   3.186102  478.613032  \n",
       "175  2.996934  451.257815  \n",
       "169  3.273997  492.265194  \n",
       "105  3.219987  483.869164  \n",
       "148  4.194746  629.493178  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ROR combos (val):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>Residual</th>\n",
       "      <th>ROR</th>\n",
       "      <th>Residual_Params</th>\n",
       "      <th>ROR_Params</th>\n",
       "      <th>VAL_RMSE</th>\n",
       "      <th>VAL_RMSPE</th>\n",
       "      <th>VAL_MAPE</th>\n",
       "      <th>VAL_MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>800.216891</td>\n",
       "      <td>5.333192</td>\n",
       "      <td>4.415575</td>\n",
       "      <td>663.169063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>807.839547</td>\n",
       "      <td>5.390755</td>\n",
       "      <td>4.184002</td>\n",
       "      <td>628.131245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'n_estimators': 200, 'loss': 'exponential', '...</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>833.188746</td>\n",
       "      <td>5.566735</td>\n",
       "      <td>4.199051</td>\n",
       "      <td>630.044386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>{'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha...</td>\n",
       "      <td>833.831394</td>\n",
       "      <td>5.526198</td>\n",
       "      <td>4.201611</td>\n",
       "      <td>633.390487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'n_estimators': 200, 'loss': 'exponential', '...</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>858.804674</td>\n",
       "      <td>5.727120</td>\n",
       "      <td>4.513954</td>\n",
       "      <td>677.757660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>{'subsample': 1.0, 'reg_lambda': 5, 'reg_alpha...</td>\n",
       "      <td>884.365809</td>\n",
       "      <td>5.876705</td>\n",
       "      <td>4.719057</td>\n",
       "      <td>710.313992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>{'random_strength': 0.0, 'n_estimators': 300, ...</td>\n",
       "      <td>889.471261</td>\n",
       "      <td>5.915476</td>\n",
       "      <td>4.768250</td>\n",
       "      <td>717.610812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>{'n_estimators': 400, 'loss': 'square', 'learn...</td>\n",
       "      <td>{'n_estimators': 200, 'loss': 'exponential', '...</td>\n",
       "      <td>896.154203</td>\n",
       "      <td>5.979347</td>\n",
       "      <td>5.089606</td>\n",
       "      <td>764.161309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>{'n_estimators': 400, 'loss': 'square', 'learn...</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>897.062715</td>\n",
       "      <td>5.981129</td>\n",
       "      <td>4.622602</td>\n",
       "      <td>694.610831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_s...</td>\n",
       "      <td>{'random_strength': 2.0, 'n_estimators': 300, ...</td>\n",
       "      <td>906.735915</td>\n",
       "      <td>6.044215</td>\n",
       "      <td>4.623665</td>\n",
       "      <td>694.829095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Base          Residual               ROR  \\\n",
       "297  GradientBoosting  GradientBoosting          CatBoost   \n",
       "303  GradientBoosting  GradientBoosting  GradientBoosting   \n",
       "239  GradientBoosting          AdaBoost  GradientBoosting   \n",
       "476          AdaBoost          CatBoost          LightGBM   \n",
       "233  GradientBoosting          AdaBoost          CatBoost   \n",
       "451          AdaBoost          CatBoost           XGBoost   \n",
       "485          AdaBoost          CatBoost          CatBoost   \n",
       "765          LightGBM          AdaBoost          AdaBoost   \n",
       "111          CatBoost          AdaBoost  GradientBoosting   \n",
       "169          CatBoost  GradientBoosting          CatBoost   \n",
       "\n",
       "                                       Residual_Params  \\\n",
       "297  {'subsample': 0.8, 'n_estimators': 200, 'min_s...   \n",
       "303  {'subsample': 0.8, 'n_estimators': 200, 'min_s...   \n",
       "239  {'n_estimators': 200, 'loss': 'exponential', '...   \n",
       "476  {'random_strength': 2.0, 'n_estimators': 300, ...   \n",
       "233  {'n_estimators': 200, 'loss': 'exponential', '...   \n",
       "451  {'random_strength': 2.0, 'n_estimators': 300, ...   \n",
       "485  {'random_strength': 2.0, 'n_estimators': 300, ...   \n",
       "765  {'n_estimators': 400, 'loss': 'square', 'learn...   \n",
       "111  {'n_estimators': 400, 'loss': 'square', 'learn...   \n",
       "169  {'subsample': 0.8, 'n_estimators': 200, 'min_s...   \n",
       "\n",
       "                                            ROR_Params    VAL_RMSE  VAL_RMSPE  \\\n",
       "297  {'random_strength': 2.0, 'n_estimators': 300, ...  800.216891   5.333192   \n",
       "303  {'subsample': 0.8, 'n_estimators': 200, 'min_s...  807.839547   5.390755   \n",
       "239  {'subsample': 0.8, 'n_estimators': 200, 'min_s...  833.188746   5.566735   \n",
       "476  {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha...  833.831394   5.526198   \n",
       "233  {'random_strength': 2.0, 'n_estimators': 300, ...  858.804674   5.727120   \n",
       "451  {'subsample': 1.0, 'reg_lambda': 5, 'reg_alpha...  884.365809   5.876705   \n",
       "485  {'random_strength': 0.0, 'n_estimators': 300, ...  889.471261   5.915476   \n",
       "765  {'n_estimators': 200, 'loss': 'exponential', '...  896.154203   5.979347   \n",
       "111  {'subsample': 0.8, 'n_estimators': 200, 'min_s...  897.062715   5.981129   \n",
       "169  {'random_strength': 2.0, 'n_estimators': 300, ...  906.735915   6.044215   \n",
       "\n",
       "     VAL_MAPE     VAL_MAE  \n",
       "297  4.415575  663.169063  \n",
       "303  4.184002  628.131245  \n",
       "239  4.199051  630.044386  \n",
       "476  4.201611  633.390487  \n",
       "233  4.513954  677.757660  \n",
       "451  4.719057  710.313992  \n",
       "485  4.768250  717.610812  \n",
       "765  5.089606  764.161309  \n",
       "111  4.622602  694.610831  \n",
       "169  4.623665  694.829095  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.3 Residual + ROR Combo Search (validation)\n",
    "resid_search_rows = []\n",
    "\n",
    "# Precompute prev prices for ROR target on train/val\n",
    "prev_train = compute_prev_prices(y, X_train.index)\n",
    "prev_val = compute_prev_prices(y, X_val.index)\n",
    "\n",
    "# Residual combos: base + residual (params grid on residual)\n",
    "base_candidates = baseline_rank_df.index.tolist() if 'baseline_rank_df' in globals() and len(baseline_rank_df) > 0 else BASELINE_MODELS\n",
    "\n",
    "for base_name in base_candidates:\n",
    "    base_train_pred = base_pred_train[base_name]\n",
    "    base_val_pred = base_pred_val[base_name]\n",
    "    resid_target_train = y_train - base_train_pred\n",
    "\n",
    "    for resid_name in RESIDUAL_MODELS:\n",
    "        for params in param_grids[resid_name]:\n",
    "            resid_model = build_model(resid_name, params)\n",
    "            resid_model.fit(X_train, resid_target_train)\n",
    "            resid_pred_val = resid_model.predict(X_val)\n",
    "            y_pred_val = base_val_pred + resid_pred_val\n",
    "            rmse, rmspe, mape, mae = eval_metrics(y_val, y_pred_val)\n",
    "\n",
    "            resid_search_rows.append({\n",
    "                'Base': base_name,\n",
    "                'Residual': resid_name,\n",
    "                'Residual_Params': params,\n",
    "                'VAL_RMSE': rmse,\n",
    "                'VAL_RMSPE': rmspe,\n",
    "                'VAL_MAPE': mape,\n",
    "                'VAL_MAE': mae\n",
    "            })\n",
    "\n",
    "resid_search_df = pd.DataFrame(resid_search_rows)\n",
    "resid_best_df = resid_search_df.sort_values('VAL_RMSE').groupby(['Base', 'Residual']).head(1)\n",
    "resid_best_df = resid_best_df.sort_values('VAL_RMSE')\n",
    "\n",
    "print(\"Top Residual combos (val):\")\n",
    "display(resid_best_df)\n",
    "\n",
    "best_resid_combo = resid_best_df.iloc[0] if len(resid_best_df) > 0 else None\n",
    "\n",
    "# ROR combos: base + residual + ror (params grid on ror)\n",
    "ror_search_rows = []\n",
    "resid_candidates = resid_best_df\n",
    "\n",
    "for _, row in resid_candidates.iterrows():\n",
    "    base_name = row['Base']\n",
    "    resid_name = row['Residual']\n",
    "    resid_params = row['Residual_Params']\n",
    "\n",
    "    base_train_pred = base_pred_train[base_name]\n",
    "    base_val_pred = base_pred_val[base_name]\n",
    "    resid_target_train = y_train - base_train_pred\n",
    "\n",
    "    # fit residual model (best params) to get train/val residual preds\n",
    "    resid_model = build_model(resid_name, resid_params)\n",
    "    resid_model.fit(X_train, resid_target_train)\n",
    "    resid_pred_train = resid_model.predict(X_train)\n",
    "    resid_pred_val = resid_model.predict(X_val)\n",
    "\n",
    "    X_train_meta = make_meta_features(X_train, base_train_pred, resid_pred_train)\n",
    "    X_val_meta = make_meta_features(X_val, base_val_pred, resid_pred_val)\n",
    "\n",
    "    ror_y_train = ror_target(y_train, prev_train, base_train_pred, resid_pred_train, mode=ROR_TARGET_MODE)\n",
    "\n",
    "    for ror_name in ROR_MODELS:\n",
    "        for ror_params in param_grids[ror_name]:\n",
    "            ror_model = build_model(ror_name, ror_params)\n",
    "            ror_model.fit(X_train_meta, ror_y_train)\n",
    "            ror_pred_val = ror_model.predict(X_val_meta)\n",
    "            y_pred_val = ror_pred_to_price(ror_pred_val, prev_val, base_val_pred, resid_pred_val, mode=ROR_TARGET_MODE)\n",
    "            rmse, rmspe, mape, mae = eval_metrics(y_val, y_pred_val)\n",
    "\n",
    "            ror_search_rows.append({\n",
    "                'Base': base_name,\n",
    "                'Residual': resid_name,\n",
    "                'ROR': ror_name,\n",
    "                'Residual_Params': resid_params,\n",
    "                'ROR_Params': ror_params,\n",
    "                'VAL_RMSE': rmse,\n",
    "                'VAL_RMSPE': rmspe,\n",
    "                'VAL_MAPE': mape,\n",
    "                'VAL_MAE': mae\n",
    "            })\n",
    "\n",
    "ror_search_df = pd.DataFrame(ror_search_rows)\n",
    "ror_best_df = ror_search_df.sort_values('VAL_RMSE').groupby(['Base', 'Residual', 'ROR']).head(1)\n",
    "ror_best_df = ror_best_df.sort_values('VAL_RMSE')\n",
    "\n",
    "print(\"Top ROR combos (val):\")\n",
    "display(ror_best_df)\n",
    "\n",
    "best_ror_combo = ror_best_df.iloc[0] if len(ror_best_df) > 0 else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Residual Diagnostics\n",
    "ëª©í‘œ: best residual comboì˜ ì”ì°¨ ë¶„í¬ì™€ ìê¸°ìƒê´€ í™•ì¸.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n",
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model for backtests: ROR_GradientBoosting+GradientBoosting+CatBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/2yp1vxc13j598z_jr_jghrz80000gn/T/ipykernel_65937/2131516933.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  preds[model_key] = pred_test\n"
     ]
    }
   ],
   "source": [
    "# 4.4 Test Evaluation (Baseline/Residual/ROR) + Residual Diagnostics\n",
    "# Prepare full train (train + val)\n",
    "X_train_full = pd.concat([X_train_all, X_val_all])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "X_train_full = X_train_full[selected_features]\n",
    "\n",
    "# prev prices for ROR on train_full/test\n",
    "prev_train_full = compute_prev_prices(y, X_train_full.index)\n",
    "prev_test = compute_prev_prices(y, X_test.index)\n",
    "\n",
    "# Baseline models (train_full -> test)\n",
    "base_pred_train_full = {}\n",
    "base_pred_test = {}\n",
    "\n",
    "for model_name in BASELINE_MODELS:\n",
    "    params = best_params_by_model[model_name]\n",
    "    model = build_model(model_name, params)\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "\n",
    "    base_pred_train_full[model_name] = model.predict(X_train_full)\n",
    "    pred_test = model.predict(X_test)\n",
    "    base_pred_test[model_name] = pred_test\n",
    "\n",
    "    model_key = f\"BASE_{model_name}\"\n",
    "    preds[model_key] = pred_test\n",
    "\n",
    "    rmse, rmspe, mape, mae = eval_metrics(y_test, pred_test)\n",
    "    results.append({\n",
    "        'Model': model_key,\n",
    "        'RMSE': rmse,\n",
    "        'RMSPE': rmspe,\n",
    "        'MAPE': mape,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "# Residual combos (top K from validation)\n",
    "resid_eval_rows = []\n",
    "best_resid_name = None\n",
    "best_resid_pred_test = None\n",
    "best_base_pred_test = None\n",
    "\n",
    "resid_candidates = resid_best_df if len(resid_best_df) > 0 else pd.DataFrame()\n",
    "for _, row in resid_candidates.iterrows():\n",
    "    base_name = row['Base']\n",
    "    resid_name = row['Residual']\n",
    "    resid_params = row['Residual_Params']\n",
    "\n",
    "    resid_target_train_full = y_train_full - base_pred_train_full[base_name]\n",
    "    resid_model = build_model(resid_name, resid_params)\n",
    "    resid_model.fit(X_train_full, resid_target_train_full)\n",
    "\n",
    "    resid_pred_test = resid_model.predict(X_test)\n",
    "    pred_test = base_pred_test[base_name] + resid_pred_test\n",
    "\n",
    "    model_key = f\"RES_{base_name}+{resid_name}\"\n",
    "    preds[model_key] = pred_test\n",
    "\n",
    "    rmse, rmspe, mape, mae = eval_metrics(y_test, pred_test)\n",
    "    results.append({\n",
    "        'Model': model_key,\n",
    "        'RMSE': rmse,\n",
    "        'RMSPE': rmspe,\n",
    "        'MAPE': mape,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "    resid_eval_rows.append({'Model': model_key, 'RMSE': rmse, 'MAE': mae})\n",
    "\n",
    "    # store best residual combo for diagnostics\n",
    "    if best_resid_combo is not None and base_name == best_resid_combo['Base'] and resid_name == best_resid_combo['Residual']:\n",
    "        best_resid_name = model_key\n",
    "        best_resid_pred_test = pred_test\n",
    "        best_base_pred_test = base_pred_test[base_name]\n",
    "\n",
    "# ROR combos (top K from validation)\n",
    "ror_eval_rows = []\n",
    "ror_candidates = ror_best_df if len(ror_best_df) > 0 else pd.DataFrame()\n",
    "\n",
    "for _, row in ror_candidates.iterrows():\n",
    "    base_name = row['Base']\n",
    "    resid_name = row['Residual']\n",
    "    ror_name = row['ROR']\n",
    "    resid_params = row['Residual_Params']\n",
    "    ror_params = row['ROR_Params']\n",
    "\n",
    "    # fit residual model\n",
    "    resid_target_train_full = y_train_full - base_pred_train_full[base_name]\n",
    "    resid_model = build_model(resid_name, resid_params)\n",
    "    resid_model.fit(X_train_full, resid_target_train_full)\n",
    "    resid_pred_train_full = resid_model.predict(X_train_full)\n",
    "    resid_pred_test = resid_model.predict(X_test)\n",
    "\n",
    "    # meta features for ror model\n",
    "    X_train_meta = make_meta_features(X_train_full, base_pred_train_full[base_name], resid_pred_train_full)\n",
    "    X_test_meta = make_meta_features(X_test, base_pred_test[base_name], resid_pred_test)\n",
    "\n",
    "    ror_y_train_full = ror_target(y_train_full, prev_train_full, base_pred_train_full[base_name], resid_pred_train_full, mode=ROR_TARGET_MODE)\n",
    "    ror_model = build_model(ror_name, ror_params)\n",
    "    ror_model.fit(X_train_meta, ror_y_train_full)\n",
    "\n",
    "    ror_pred_test = ror_model.predict(X_test_meta)\n",
    "    pred_test = ror_pred_to_price(ror_pred_test, prev_test, base_pred_test[base_name], resid_pred_test, mode=ROR_TARGET_MODE)\n",
    "\n",
    "    model_key = f\"ROR_{base_name}+{resid_name}+{ror_name}\"\n",
    "    preds[model_key] = pred_test\n",
    "\n",
    "    rmse, rmspe, mape, mae = eval_metrics(y_test, pred_test)\n",
    "    results.append({\n",
    "        'Model': model_key,\n",
    "        'RMSE': rmse,\n",
    "        'RMSPE': rmspe,\n",
    "        'MAPE': mape,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "    ror_eval_rows.append({'Model': model_key, 'RMSE': rmse, 'MAE': mae})\n",
    "\n",
    "# Select model for downstream backtests\n",
    "if best_ror_combo is not None:\n",
    "    selected_model_name = f\"ROR_{best_ror_combo['Base']}+{best_ror_combo['Residual']}+{best_ror_combo['ROR']}\"\n",
    "elif best_resid_combo is not None:\n",
    "    selected_model_name = f\"RES_{best_resid_combo['Base']}+{best_resid_combo['Residual']}\"\n",
    "elif best_baseline_model is not None:\n",
    "    selected_model_name = f\"BASE_{best_baseline_model}\"\n",
    "else:\n",
    "    selected_model_name = \"Naive_Last\"\n",
    "\n",
    "print(f\"Selected model for backtests: {selected_model_name}\")\n",
    "\n",
    "# Residual diagnostics for best residual combo (if available)\n",
    "if best_resid_name is not None:\n",
    "    base_residuals = y_test - best_base_pred_test\n",
    "    resid_residuals = y_test - best_resid_pred_test\n",
    "\n",
    "    base_rmse, base_rmspe, base_mape, base_mae = eval_metrics(y_test, best_base_pred_test)\n",
    "    res_rmse, res_rmspe, res_mape, res_mae = eval_metrics(y_test, best_resid_pred_test)\n",
    "\n",
    "    print(\"Residual Correction: Base vs Residual (Best Combo)\")\n",
    "    print(f\"Base   RMSE={base_rmse:.4f}, RMSPE={base_rmspe:.2f}, MAPE={base_mape:.2f}, MAE={base_mae:.4f}\")\n",
    "    print(f\"Resid. RMSE={res_rmse:.4f}, RMSPE={res_rmspe:.2f}, MAPE={res_mape:.2f}, MAE={res_mae:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(base_residuals, bins=20, alpha=0.6, label='Base Residuals')\n",
    "    plt.hist(resid_residuals, bins=20, alpha=0.6, label='Residual-Boost Residuals')\n",
    "    plt.title('Residual Distribution (Test)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Simple ACF\n",
    "    max_lag = min(12, len(base_residuals) - 2)\n",
    "    if max_lag < 1:\n",
    "        print(\"Residual series too short for ACF.\")\n",
    "    else:\n",
    "        lags = range(1, max_lag + 1)\n",
    "\n",
    "        def acf_values(residuals, lags):\n",
    "            vals = []\n",
    "            r = residuals.values\n",
    "            r = r - np.mean(r)\n",
    "            for lag in lags:\n",
    "                v = np.corrcoef(r[lag:], r[:-lag])[0, 1]\n",
    "                vals.append(v)\n",
    "            return vals\n",
    "\n",
    "        base_acf = acf_values(base_residuals, lags)\n",
    "        res_acf = acf_values(resid_residuals, lags)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar([l - 0.15 for l in lags], base_acf, width=0.3, label='Base')\n",
    "        plt.bar([l + 0.15 for l in lags], res_acf, width=0.3, label='Residual-Boost')\n",
    "        plt.axhline(0, color='black', linewidth=0.8)\n",
    "        plt.title('Residual ACF (Test)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Finalize results ordering\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "order = []\n",
    "order += [f\"BASE_{m}\" for m in BASELINE_MODELS]\n",
    "order += [f\"RES_{r['Base']}+{r['Residual']}\" for _, r in resid_candidates.iterrows()] if len(resid_candidates) > 0 else []\n",
    "order += [f\"ROR_{r['Base']}+{r['Residual']}+{r['ROR']}\" for _, r in ror_candidates.iterrows()] if len(ror_candidates) > 0 else []\n",
    "order += ['Naive_Last', 'Naive_Drift']\n",
    "\n",
    "order = [m for m in order if m in results_df.index]\n",
    "results_df = results_df.loc[order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4.5 Baseline Prediction Comparison (Train/Val/Test)\n",
    "# Train/Val: predictions from train-only fit (best params per model)\n",
    "# Test: predictions from train+val fit (best params per model)\n",
    "\n",
    "def _plot_baseline_split(split_name, y_true, pred_dict):\n",
    "    n = len(BASELINE_MODELS)\n",
    "    cols = 2\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 3.5 * rows), sharex=True)\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "    for ax, name in zip(axes, BASELINE_MODELS):\n",
    "        ax.plot(y_true.index, y_true.values, label='Actual', color='black', linewidth=1.2)\n",
    "        ax.plot(y_true.index, pred_dict[name], label=f'Pred ({name})', color='red', linestyle='--')\n",
    "        ax.set_title(f\"{split_name}: {name}\")\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    for ax in axes[len(BASELINE_MODELS):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "_plot_baseline_split('Train', y_train, base_pred_train)\n",
    "_plot_baseline_split('Validation', y_val, base_pred_val)\n",
    "_plot_baseline_split('Test', y_test, base_pred_test)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backtesting & Directional Evaluation\n",
    "ëª©í‘œ: íŠ¸ë ˆì´ë”© ê´€ì  ì„±ê³¼(ROR)ì™€ ë°©í–¥ì„± ì •í™•ë„ í‰ê°€.\n",
    "í•´ì„: ìˆ˜ìµê³¼ ê±°ë˜ íšŸìˆ˜ì˜ ê· í˜•ì„ í™•ì¸.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ROR Backtesting & Threshold Sweep\n",
    "ëª©í‘œ: í…ŒìŠ¤íŠ¸ ê¸°ê°„ ê¸°ì¤€ ì„ê³„ê°’ë³„ ê±°ë˜ìˆ˜ì™€ ëˆ„ì  ROR ë¹„êµ.\n",
    "í•´ì„: thresholdì— ë¯¼ê°í•œ ëª¨ë¸ì€ ì•ˆì •ì„± ë‚®ìŒ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c78dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROR Summary (Test period, best threshold per model):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Trades</th>\n",
       "      <th>Cumulative_ROR</th>\n",
       "      <th>Avg_ROR</th>\n",
       "      <th>Win_Rate</th>\n",
       "      <th>Avg_Win</th>\n",
       "      <th>Avg_Loss</th>\n",
       "      <th>Profit_Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Naive_Drift</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.128877</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.031065</td>\n",
       "      <td>-0.029526</td>\n",
       "      <td>2.454947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Naive_Last</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>ROR_XGBoost+XGBoost+AdaBoost</td>\n",
       "      <td>0.003</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.983275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>ROR_LightGBM+LightGBM+LightGBM</td>\n",
       "      <td>0.003</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.983275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>ROR_XGBoost+XGBoost+GradientBoosting</td>\n",
       "      <td>0.003</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.983275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>ROR_LightGBM+XGBoost+LightGBM</td>\n",
       "      <td>0.003</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.953748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>ROR_XGBoost+LightGBM+CatBoost</td>\n",
       "      <td>0.005</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.019380</td>\n",
       "      <td>-0.001615</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.880720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>ROR_AdaBoost+AdaBoost+LightGBM</td>\n",
       "      <td>0.003</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.876314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>ROR_AdaBoost+XGBoost+LightGBM</td>\n",
       "      <td>0.003</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.021577</td>\n",
       "      <td>-0.001798</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.018791</td>\n",
       "      <td>-0.042977</td>\n",
       "      <td>0.874485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>ROR_AdaBoost+GradientBoosting+LightGBM</td>\n",
       "      <td>0.003</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.021577</td>\n",
       "      <td>-0.001798</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.018791</td>\n",
       "      <td>-0.042977</td>\n",
       "      <td>0.874485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model  Threshold  Trades  \\\n",
       "469                             Naive_Drift      0.005      10   \n",
       "467                              Naive_Last      0.010       0   \n",
       "447            ROR_XGBoost+XGBoost+AdaBoost      0.003      12   \n",
       "339          ROR_LightGBM+LightGBM+LightGBM      0.003      12   \n",
       "429    ROR_XGBoost+XGBoost+GradientBoosting      0.003      12   \n",
       "297           ROR_LightGBM+XGBoost+LightGBM      0.003      11   \n",
       "421           ROR_XGBoost+LightGBM+CatBoost      0.005      11   \n",
       "240          ROR_AdaBoost+AdaBoost+LightGBM      0.003      11   \n",
       "366           ROR_AdaBoost+XGBoost+LightGBM      0.003      12   \n",
       "312  ROR_AdaBoost+GradientBoosting+LightGBM      0.003      12   \n",
       "\n",
       "     Cumulative_ROR   Avg_ROR  Win_Rate   Avg_Win  Avg_Loss  Profit_Factor  \n",
       "469        0.128877  0.010740  0.700000  0.031065 -0.029526       2.454947  \n",
       "467        0.000000  0.000000       NaN  0.000000  0.000000            inf  \n",
       "447       -0.002717 -0.000226  0.666667  0.019970 -0.040619       0.983275  \n",
       "339       -0.002717 -0.000226  0.666667  0.019970 -0.040619       0.983275  \n",
       "429       -0.002717 -0.000226  0.666667  0.019970 -0.040619       0.983275  \n",
       "297       -0.007515 -0.000626  0.636364  0.022137 -0.040619       0.953748  \n",
       "421       -0.019380 -0.001615  0.636364  0.020442 -0.040619       0.880720  \n",
       "240       -0.020096 -0.001675  0.636364  0.020340 -0.040619       0.876314  \n",
       "366       -0.021577 -0.001798  0.666667  0.018791 -0.042977       0.874485  \n",
       "312       -0.021577 -0.001798  0.666667  0.018791 -0.042977       0.874485  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model Trades: 12 / 12\n",
      "Selected Model Cumulative ROR: -0.0913\n"
     ]
    }
   ],
   "source": [
    "# 5. Backtesting & Rate of Return (ROR) on test period\n",
    "thresholds = [0.003, 0.005, 0.01]\n",
    "ror_rows = []\n",
    "\n",
    "prev_prices = y_test.shift(1).copy()\n",
    "last_before_test = y.loc[y.index < y_test.index[0]]\n",
    "if len(last_before_test) < 1:\n",
    "    raise ValueError(\"Not enough history before test for ROR backtest.\")\n",
    "prev_prices.iloc[0] = last_before_test.iloc[-1]\n",
    "\n",
    "for name in results_df.index:\n",
    "    if name not in preds.columns:\n",
    "        continue\n",
    "    pred_prices = preds[name]\n",
    "    actual_returns = (y_test / prev_prices) - 1\n",
    "    pred_returns = (pred_prices / prev_prices) - 1\n",
    "\n",
    "    for thr in thresholds:\n",
    "        ror_all = []\n",
    "        for pred_ret, actual_ret in zip(pred_returns.values, actual_returns.values):\n",
    "            if pred_ret > thr:\n",
    "                ror_all.append(actual_ret)\n",
    "            elif pred_ret < -thr:\n",
    "                ror_all.append(-actual_ret)\n",
    "            else:\n",
    "                ror_all.append(0.0)\n",
    "\n",
    "        cum_ror = np.cumsum(ror_all)\n",
    "        trade_count = int(np.sum(np.abs(pred_returns.values) > thr))\n",
    "        trades = [r for r in ror_all if r != 0]\n",
    "        wins = [r for r in trades if r > 0]\n",
    "        losses = [r for r in trades if r < 0]\n",
    "        win_rate = len(wins) / len(trades) if trades else np.nan\n",
    "        avg_win = float(np.mean(wins)) if wins else 0.0\n",
    "        avg_loss = float(np.mean(losses)) if losses else 0.0\n",
    "        profit_factor = (sum(wins) / abs(sum(losses))) if losses else np.inf\n",
    "\n",
    "        ror_rows.append({\n",
    "            'Model': name,\n",
    "            'Threshold': thr,\n",
    "            'Trades': trade_count,\n",
    "            'Cumulative_ROR': float(cum_ror[-1]),\n",
    "            'Avg_ROR': float(np.mean(ror_all)),\n",
    "            'Win_Rate': win_rate,\n",
    "            'Avg_Win': avg_win,\n",
    "            'Avg_Loss': avg_loss,\n",
    "            'Profit_Factor': profit_factor\n",
    "        })\n",
    "\n",
    "ror_df = pd.DataFrame(ror_rows)\n",
    "print(\"ROR Summary (Test period):\")\n",
    "display(ror_df)\n",
    "\n",
    "# Heatmaps for threshold sweep\n",
    "pivot_ror = ror_df.pivot(index='Model', columns='Threshold', values='Cumulative_ROR').loc[results_df.index]\n",
    "pivot_trades = ror_df.pivot(index='Model', columns='Threshold', values='Trades').loc[results_df.index]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(pivot_ror, annot=True, fmt='.3f', cmap='coolwarm', center=0)\n",
    "plt.title('Cumulative ROR by Model/Threshold')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(pivot_trades, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Trade Count by Model/Threshold')\n",
    "plt.show()\n",
    "\n",
    "# Cumulative ROR curve for selected model at default threshold\n",
    "trade_threshold = 0.005\n",
    "pred_returns = (preds[selected_model_name] / prev_prices) - 1\n",
    "actual_returns = (y_test / prev_prices) - 1\n",
    "\n",
    "ror_all = []\n",
    "for pred_ret, actual_ret in zip(pred_returns.values, actual_returns.values):\n",
    "    if pred_ret > trade_threshold:\n",
    "        ror_all.append(actual_ret)\n",
    "    elif pred_ret < -trade_threshold:\n",
    "        ror_all.append(-actual_ret)\n",
    "    else:\n",
    "        ror_all.append(0.0)\n",
    "\n",
    "cum_ror = np.cumsum(ror_all)\n",
    "trade_count = int(np.sum(np.abs(pred_returns.values) > trade_threshold))\n",
    "print(f\"Selected Model Trades: {trade_count} / {len(y_test)}\")\n",
    "print(f\"Selected Model Cumulative ROR: {cum_ror[-1]:.4f}\")\n",
    "\n",
    "cum_ror_series = pd.Series(cum_ror, index=y_test.index)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(cum_ror_series.index, cum_ror_series.values, color='purple')\n",
    "plt.axhline(0, color='black', linewidth=0.8)\n",
    "plt.title(f'Cumulative ROR (Test period): {selected_model_name}')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Test Period Price Backtest\n",
    "ëª©í‘œ: í…ŒìŠ¤íŠ¸ ê¸°ê°„(2025-10-27~2026-01-12) ê°€ê²© ë³µì›ìœ¼ë¡œ ì‹¤ì „ ê·¼ì‚¬ í‰ê°€.\n",
    "í•´ì„: ì¶”ì„¸ì™€ ë ˆë²¨ ì¬í˜„ ì—¬ë¶€ í™•ì¸.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc3bef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE for test period (price-level): 10.70%\n",
      "Weekly Forecast Schedule (last rows):\n",
      "2025-12-08: 14941.40\n",
      "2025-12-15: 15125.34\n",
      "2025-12-22: 14521.07\n",
      "2025-12-29: 12910.85\n",
      "2026-01-05: 14062.13\n",
      "2026-01-12: 15706.61\n"
     ]
    }
   ],
   "source": [
    "# 6. Test Period Backtest (evaluate only on the test dates)\n",
    "last_12_dates = y_test.index\n",
    "actual_prices = y_test\n",
    "predicted_prices = preds[selected_model_name]\n",
    "\n",
    "mape = np.mean(np.abs((actual_prices - predicted_prices) / actual_prices)) * 100\n",
    "print(f\"MAPE for test period (price-level): {mape:.2f}%\")\n",
    "print(\"Weekly Forecast Schedule:\")\n",
    "for d, p in zip(last_12_dates, predicted_prices):\n",
    "    print(f\"{d.date()}: {p:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(last_12_dates, actual_prices, label='Actual', color='black')\n",
    "plt.plot(last_12_dates, predicted_prices, label='Predicted', color='red', linestyle='--')\n",
    "plt.title(f'Actual vs Predicted Price (Test Period): {selected_model_name}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 6.1 Representative Model Comparison (Test)\n",
    "rep_models = []\n",
    "if best_baseline_model is not None:\n",
    "    rep_models.append(f\"BASE_{best_baseline_model}\")\n",
    "if selected_model_name in preds.columns and selected_model_name not in rep_models:\n",
    "    rep_models.append(selected_model_name)\n",
    "\n",
    "if rep_models:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(preds.index, preds['Actual'], label='Actual', color='black', linewidth=1.2)\n",
    "    for name in rep_models:\n",
    "        if name in preds.columns:\n",
    "            plt.plot(preds.index, preds[name], label=name, linestyle='--')\n",
    "    plt.title('Representative Model Comparison (Test)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No representative models available for comparison.')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Directional Evaluation\n",
    "ëª©í‘œ: ìƒìŠ¹/í•˜ë½ ë°©í–¥ ì •í™•ë„ ë° í˜¼ë™í–‰ë ¬ ë¶„ì„.\n",
    "í•´ì„: ë°©í–¥ ë§ì¶¤ì´ íŠ¸ë ˆì´ë”© ì„±ê³¼ì™€ ì§ê²°.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up hit: 3, Down hit: 2, Directional acc: 41.67%\n"
     ]
    }
   ],
   "source": [
    "# 7. Directional evaluation (up/down)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "def to_dir(s, thr=0.0):\n",
    "    return np.where(s > thr, 1, np.where(s < -thr, -1, 0))\n",
    "\n",
    "direction_threshold = 0.0  # set to 0.005 to match trading threshold\n",
    "\n",
    "prev_prices = y_test.shift(1).copy()\n",
    "last_before_test = y.loc[y.index < y_test.index[0]]\n",
    "if len(last_before_test) < 1:\n",
    "    raise ValueError(\"Not enough history before test for directional eval.\")\n",
    "prev_prices.iloc[0] = last_before_test.iloc[-1]\n",
    "\n",
    "actual_returns = (y_test / prev_prices) - 1\n",
    "pred_returns = (preds[selected_model_name] / prev_prices) - 1\n",
    "\n",
    "y_true = to_dir(actual_returns, direction_threshold)\n",
    "y_pred = to_dir(pred_returns, direction_threshold)\n",
    "\n",
    "# Exclude neutral moves (0) to focus on up/down accuracy\n",
    "mask = (y_true != 0) & (y_pred != 0)\n",
    "if mask.sum() == 0:\n",
    "    print(f'No directional signals at threshold {direction_threshold}.')\n",
    "else:\n",
    "    cm = confusion_matrix(y_true[mask], y_pred[mask], labels=[1, -1])\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Up', 'Down'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f'Directional Confusion Matrix: {selected_model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    up_hit, up_miss = cm[0, 0], cm[0, 1]\n",
    "    down_miss, down_hit = cm[1, 0], cm[1, 1]\n",
    "    directional_acc = (up_hit + down_hit) / cm.sum() if cm.sum() else 0\n",
    "    print(f'Up hit: {up_hit}, Down hit: {down_hit}, Directional acc: {directional_acc:.2%}')\n",
    "    print(classification_report(y_true[mask], y_pred[mask], labels=[1, -1], target_names=['Up', 'Down'], zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Results\n",
    "ëª©í‘œ: ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë™ì¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ë¹„êµ.\n",
    "í•´ì„: í…Œì´ë¸”ê³¼ Actual vs Predicted ê·¸ë¦¬ë“œë¡œ ì¢…í•© íŒë‹¨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results (Test, sorted by RMSE):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSPE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive_Drift</th>\n",
       "      <td>480.665760</td>\n",
       "      <td>3.070722</td>\n",
       "      <td>2.103092</td>\n",
       "      <td>325.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Last</th>\n",
       "      <td>569.225486</td>\n",
       "      <td>3.498880</td>\n",
       "      <td>2.577581</td>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROR_GradientBoosting+CatBoost+LightGBM</th>\n",
       "      <td>1264.727697</td>\n",
       "      <td>7.694806</td>\n",
       "      <td>6.010937</td>\n",
       "      <td>949.906953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROR_CatBoost+CatBoost+LightGBM</th>\n",
       "      <td>1274.172906</td>\n",
       "      <td>7.800692</td>\n",
       "      <td>6.064709</td>\n",
       "      <td>953.950667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROR_CatBoost+CatBoost+XGBoost</th>\n",
       "      <td>1304.588216</td>\n",
       "      <td>7.936814</td>\n",
       "      <td>6.012772</td>\n",
       "      <td>951.404507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BASE_GradientBoosting</th>\n",
       "      <td>1312.088798</td>\n",
       "      <td>7.688035</td>\n",
       "      <td>5.732709</td>\n",
       "      <td>937.618658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RES_GradientBoosting+CatBoost</th>\n",
       "      <td>1317.935544</td>\n",
       "      <td>7.869677</td>\n",
       "      <td>6.471181</td>\n",
       "      <td>1040.428755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RES_CatBoost+CatBoost</th>\n",
       "      <td>1327.886091</td>\n",
       "      <td>8.050487</td>\n",
       "      <td>6.450306</td>\n",
       "      <td>1028.772883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BASE_CatBoost</th>\n",
       "      <td>1332.729332</td>\n",
       "      <td>7.912486</td>\n",
       "      <td>6.223443</td>\n",
       "      <td>1005.846161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RES_CatBoost+AdaBoost</th>\n",
       "      <td>1369.554492</td>\n",
       "      <td>8.143938</td>\n",
       "      <td>6.543399</td>\n",
       "      <td>1055.678960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               RMSE     RMSPE      MAPE  \\\n",
       "Model                                                                     \n",
       "Naive_Drift                              480.665760  3.070722  2.103092   \n",
       "Naive_Last                               569.225486  3.498880  2.577581   \n",
       "ROR_GradientBoosting+CatBoost+LightGBM  1264.727697  7.694806  6.010937   \n",
       "ROR_CatBoost+CatBoost+LightGBM          1274.172906  7.800692  6.064709   \n",
       "ROR_CatBoost+CatBoost+XGBoost           1304.588216  7.936814  6.012772   \n",
       "BASE_GradientBoosting                   1312.088798  7.688035  5.732709   \n",
       "RES_GradientBoosting+CatBoost           1317.935544  7.869677  6.471181   \n",
       "RES_CatBoost+CatBoost                   1327.886091  8.050487  6.450306   \n",
       "BASE_CatBoost                           1332.729332  7.912486  6.223443   \n",
       "RES_CatBoost+AdaBoost                   1369.554492  8.143938  6.543399   \n",
       "\n",
       "                                                MAE  \n",
       "Model                                                \n",
       "Naive_Drift                              325.763889  \n",
       "Naive_Last                               410.000000  \n",
       "ROR_GradientBoosting+CatBoost+LightGBM   949.906953  \n",
       "ROR_CatBoost+CatBoost+LightGBM           953.950667  \n",
       "ROR_CatBoost+CatBoost+XGBoost            951.404507  \n",
       "BASE_GradientBoosting                    937.618658  \n",
       "RES_GradientBoosting+CatBoost           1040.428755  \n",
       "RES_CatBoost+CatBoost                   1028.772883  \n",
       "BASE_CatBoost                           1005.846161  \n",
       "RES_CatBoost+AdaBoost                   1055.678960  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model (Val): ROR_GradientBoosting+GradientBoosting+CatBoost\n",
      "ìš”ì•½ í•´ì„:\n",
      "- Baseline best (val): GradientBoosting\n",
      "- Best Residual combo (val): AdaBoost + AdaBoost\n",
      "- Best ROR combo (val): GradientBoosting + GradientBoosting + CatBoost\n",
      "- Test RMSE ìµœìš°ìˆ˜ ëª¨ë¸: Naive_Drift (RMSE=480.6658)\n",
      "- í”¼ì²˜ ì•ˆì •ì„± í‰ê·  Jaccard: 0.714\n",
      "- ì„ íƒ ëª¨ë¸ ROR ìµœê³  ì„ê³„ê°’: 0.003 (Cum ROR=-0.0913, Trades=12)\n",
      "- ROR_MODE: pred_minus_residual\n",
      "- Split: Train<2025-08-04, Val=2025-08-04~2025-10-20, Test=2025-10-27~2026-01-12\n",
      "- RMSPE/MAPEëŠ” ë¹„ìœ¨ ì§€í‘œì´ë¯€ë¡œ RMSE/MAEì™€ í•¨ê»˜ í•´ì„ ê¶Œì¥\n",
      "- Transformer baselineì€ dl_lstm_transformer.ipynb ì°¸ê³ \n"
     ]
    }
   ],
   "source": [
    "# 8. Results & Plots\n",
    "fmt = {\n",
    "    'RMSE': '{:.4f}',\n",
    "    'RMSPE': '{:.2f}',\n",
    "    'MAPE': '{:.2f}',\n",
    "    'MAE': '{:.4f}'\n",
    "}\n",
    "display(results_df.style.background_gradient(cmap='viridis', subset=['RMSE', 'RMSPE', 'MAPE', 'MAE']).format(fmt))\n",
    "print(f\"Selected Model (Val): {selected_model_name}\")\n",
    "\n",
    "# Visualization: Actual vs Predicted (Price-level, All Models)\n",
    "plot_names = list(results_df.index)\n",
    "missing = [n for n in plot_names if n not in preds.columns]\n",
    "if missing:\n",
    "    print(f\"Missing predictions for: {missing}\")\n",
    "\n",
    "n = len(plot_names)\n",
    "cols = 2\n",
    "rows = int(np.ceil(n / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows), sharex=True)\n",
    "axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "for ax, name in zip(axes, plot_names):\n",
    "    ax.plot(preds.index, preds['Actual'], label='Actual', color='black', linewidth=1.2)\n",
    "    ax.plot(preds.index, preds[name], label=f'Predicted ({name})', color='red', linestyle='--')\n",
    "    ax.set_title(f'Price: Actual vs Predicted ({name})')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "for ax in axes[len(plot_names):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Visualization: Model Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "results_df[['RMSE', 'RMSPE', 'MAPE', 'MAE']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Error Metrics Comparison (Test)')\n",
    "ax.set_ylabel('Error Value')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Summary Interpretation\n",
    "holdout_best_model = results_df['RMSE'].idxmin()\n",
    "holdout_best_rmse = results_df.loc[holdout_best_model, 'RMSE']\n",
    "\n",
    "baseline_best = best_baseline_model if 'best_baseline_model' in globals() else None\n",
    "\n",
    "sel_ror = ror_df[ror_df['Model'] == selected_model_name].sort_values('Cumulative_ROR', ascending=False).head(1)\n",
    "if not sel_ror.empty:\n",
    "    best_thr = sel_ror['Threshold'].iloc[0]\n",
    "    best_ror = sel_ror['Cumulative_ROR'].iloc[0]\n",
    "    best_trades = int(sel_ror['Trades'].iloc[0])\n",
    "else:\n",
    "    best_thr, best_ror, best_trades = None, None, None\n",
    "\n",
    "print('ìš”ì•½ í•´ì„:')\n",
    "if baseline_best is not None:\n",
    "    print(f\"- Baseline best (val): {baseline_best}\")\n",
    "if best_resid_combo is not None:\n",
    "    print(f\"- Best Residual combo (val): {best_resid_combo['Base']} + {best_resid_combo['Residual']}\")\n",
    "if best_ror_combo is not None:\n",
    "    print(f\"- Best ROR combo (val): {best_ror_combo['Base']} + {best_ror_combo['Residual']} + {best_ror_combo['ROR']}\")\n",
    "print(f\"- Test RMSE ìµœìš°ìˆ˜ ëª¨ë¸: {holdout_best_model} (RMSE={holdout_best_rmse:.4f})\")\n",
    "print(f\"- í”¼ì²˜ ì•ˆì •ì„± í‰ê·  Jaccard: {avg_jaccard:.3f}\")\n",
    "if best_thr is not None:\n",
    "    print(f\"- ì„ íƒ ëª¨ë¸ ROR ìµœê³  ì„ê³„ê°’: {best_thr} (Cum ROR={best_ror:.4f}, Trades={best_trades})\")\n",
    "print(f\"- ROR_MODE: {ROR_TARGET_MODE}\")\n",
    "print(f\"- Split: Train<2025-08-04, Val=2025-08-04~2025-10-20, Test=2025-10-27~2026-01-12\")\n",
    "print('- RMSPE/MAPEëŠ” ë¹„ìœ¨ ì§€í‘œì´ë¯€ë¡œ RMSE/MAEì™€ í•¨ê»˜ í•´ì„ ê¶Œì¥')\n",
    "print('- Transformer baselineì€ dl_lstm_transformer.ipynb ì°¸ê³ ')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}