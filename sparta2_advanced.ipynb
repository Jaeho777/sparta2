{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë‹ˆì¼ˆ ê°€ê²© ì˜ˆì¸¡ - Transformer ëª¨ë¸ ì‹¬í™” ë¶„ì„\n\n",
    "---\n\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "ê¸°ì¡´ ë¶„ì„(sparta2.ipynb)ê³¼ **ë™ì¼í•œ ì¡°ê±´**ì—ì„œ Transformer ëª¨ë¸ë¡œ ì„±ëŠ¥ í–¥ìƒ ì‹œë„\n\n",
    "## ğŸ“‹ ê¸°ì¡´ ë¶„ì„ ì¸ì‚¬ì´íŠ¸\n",
    "| í•­ëª© | ë‚´ìš© |\n",
    "|------|------|\n",
    "| ìµœê³  ì„±ëŠ¥ | Hybrid (Naive 80% + GB 20%), RMSE ~407 |\n",
    "| Naive_Drift | RMSE ~481 (ML ëª¨ë¸ ì••ë„) |\n",
    "| ML ëª¨ë¸ | RMSE 1100+ (Test ê¸°ê°„ ê¸‰ë“±ì— ëŒ€ì‘ ì‹¤íŒ¨) |\n",
    "| ì›ì¸ | Train(í‰ê· íšŒê·€) vs Test(ì¼ë°©ì  ìƒìŠ¹) íŒ¨í„´ ë¶ˆì¼ì¹˜ |\n\n",
    "## ğŸ“Š Transformer ì‹¤í—˜ ì „ëµ\n",
    "1. **ì‹œê³„ì—´ Transformer ì•„í‚¤í…ì²˜** êµ¬í˜„\n",
    "2. **ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´** ì‹¤í—˜ (4, 8, 12, 24ì£¼)\n",
    "3. **Attention ë¶„ì„**ìœ¼ë¡œ ëª¨ë¸ í•´ì„\n",
    "4. **Naive + Transformer í•˜ì´ë¸Œë¦¬ë“œ**\n",
    "5. **ì•™ìƒë¸”** (Transformer + GBM)\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë¸ (ë¹„êµìš©)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# ì„¤ì •\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# GPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 1. ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)\n\n",
    "**ë™ì¼í•œ ì¡°ê±´ ìœ ì§€:**\n",
    "- ë°ì´í„° ì†ŒìŠ¤, ì „ì²˜ë¦¬, ë¶„í•  ê¸°ê°„ ëª¨ë‘ ê¸°ì¡´ê³¼ ë™ì¼\n",
    "- ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ í”¼ì²˜ ì„ íƒë„ ë™ì¼í•˜ê²Œ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df_raw = pd.read_csv('data_weekly_260120.csv')\n",
    "df_raw['dt'] = pd.to_datetime(df_raw['dt'])\n",
    "df_raw = df_raw.set_index('dt').sort_index()\n",
    "\n",
    "# ê´€ë ¨ ì»¬ëŸ¼ í•„í„°ë§ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "target_col = 'Com_LME_Ni_Cash'\n",
    "metals = ['Gold', 'Silver', 'Iron', 'Steel', 'Copper', 'Aluminum', 'Zinc', 'Nickel', 'Lead', 'Tin', 'Uranium']\n",
    "\n",
    "filtered_cols = [target_col]\n",
    "for col in df_raw.columns:\n",
    "    if any(x in col for x in ['Idx_', 'Bonds_', 'EX_']):\n",
    "        filtered_cols.append(col)\n",
    "    elif 'Com_LME' in col:\n",
    "        filtered_cols.append(col)\n",
    "    elif any(m in col for m in metals):\n",
    "        filtered_cols.append(col)\n",
    "\n",
    "filtered_cols = sorted(list(set(filtered_cols)))\n",
    "df = df_raw[filtered_cols].copy()\n",
    "df = df.ffill().bfill()\n",
    "\n",
    "print(f'ë°ì´í„° í¬ê¸°: {df.shape}')\n",
    "print(f'ê¸°ê°„: {df.index.min().date()} ~ {df.index.max().date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬ + 1ì£¼ ì§€ì—° (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "price_full = df[target_col]\n",
    "y = price_full\n",
    "X = df.drop(columns=[target_col]).shift(1)  # ëˆ„ìˆ˜ ë°©ì§€\n",
    "\n",
    "# ì •ë ¬\n",
    "valid_idx = X.dropna().index.intersection(y.dropna().index)\n",
    "X = X.loc[valid_idx]\n",
    "y = y.loc[valid_idx]\n",
    "\n",
    "print(f'ìƒ˜í”Œ ìˆ˜: {len(y)}')\n",
    "print(f'í”¼ì²˜ ìˆ˜: {X.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test ë¶„í•  (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "val_start = pd.to_datetime('2025-08-04')\n",
    "val_end = pd.to_datetime('2025-10-20')\n",
    "test_start = pd.to_datetime('2025-10-27')\n",
    "\n",
    "train_mask = X.index < val_start\n",
    "val_mask = (X.index >= val_start) & (X.index <= val_end)\n",
    "test_mask = X.index >= test_start\n",
    "\n",
    "X_train_all, y_train = X[train_mask], y[train_mask]\n",
    "X_val_all, y_val = X[val_mask], y[val_mask]\n",
    "X_test_all, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "print(f'Train: {len(y_train)} samples ({y_train.index.min().date()} ~ {y_train.index.max().date()})')\n",
    "print(f'Val: {len(y_val)} samples')\n",
    "print(f'Test: {len(y_test)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 SHAP ê¸°ë°˜ í”¼ì²˜ ì„ íƒ (ê¸°ì¡´ê³¼ ë™ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP í”¼ì²˜ ì„ íƒ (Train only)\n",
    "top_n = 20\n",
    "\n",
    "model_shap = xgb.XGBRegressor(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "model_shap.fit(X_train_all, y_train)\n",
    "\n",
    "explainer = shap.TreeExplainer(model_shap)\n",
    "shap_values = explainer.shap_values(X_train_all)\n",
    "importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': X_train_all.columns,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# LME Index ì œì™¸ (ìˆœí™˜ë…¼ë¦¬ ë°©ì§€)\n",
    "selected_features = [f for f in feat_imp.head(top_n)['feature'].tolist() if 'Com_LME_Index' not in f]\n",
    "\n",
    "X_train = X_train_all[selected_features]\n",
    "X_val = X_val_all[selected_features]\n",
    "X_test = X_test_all[selected_features]\n",
    "\n",
    "print(f'ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(selected_features)}')\n",
    "print('\\nìƒìœ„ 10ê°œ:')\n",
    "for i, f in enumerate(selected_features[:10], 1):\n",
    "    print(f'  {i}. {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í•  ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "ax.plot(y.index, y.values, 'k-', linewidth=0.8, alpha=0.7)\n",
    "ax.axvspan(y_train.index.min(), y_train.index.max(), alpha=0.2, color='blue', label=f'Train ({len(y_train)})')\n",
    "ax.axvspan(y_val.index.min(), y_val.index.max(), alpha=0.2, color='green', label=f'Val ({len(y_val)})')\n",
    "ax.axvspan(y_test.index.min(), y_test.index.max(), alpha=0.2, color='red', label=f'Test ({len(y_test)})')\n",
    "\n",
    "ax.set_ylabel('Nickel Price (USD/ton)')\n",
    "ax.set_title('Data Split (Same as Original Analysis)', fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 2. Transformer ëª¨ë¸ êµ¬í˜„\n\n",
    "### 2.1 ì‹œê³„ì—´ Transformer ì•„í‚¤í…ì²˜\n\n",
    "```\n",
    "Input (seq_len, n_features)\n",
    "        â†“\n",
    "Linear Projection (d_model)\n",
    "        â†“\n",
    "Positional Encoding\n",
    "        â†“\n",
    "Transformer Encoder (N layers)\n",
    "  - Multi-Head Self-Attention\n",
    "  - Feed-Forward Network\n",
    "        â†“\n",
    "Global Average Pooling\n",
    "        â†“\n",
    "Output Layer â†’ Price Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ìš© Positional Encoding\n",
    "    ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ ì˜ˆì¸¡ìš© Transformer\n",
    "    \n",
    "    Args:\n",
    "        n_features: ì…ë ¥ í”¼ì²˜ ìˆ˜\n",
    "        d_model: Transformer ë‚´ë¶€ ì°¨ì›\n",
    "        nhead: Attention head ìˆ˜\n",
    "        num_layers: Encoder layer ìˆ˜\n",
    "        dim_feedforward: FFN ì°¨ì›\n",
    "        dropout: Dropout ë¹„ìœ¨\n",
    "        seq_len: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=64, nhead=4, num_layers=2, \n",
    "                 dim_feedforward=128, dropout=0.1, seq_len=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len+10, dropout=dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Attention weights ì €ì¥ìš©\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, n_features)\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_proj(x)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Global average pooling (ë§ˆì§€ë§‰ ì‹œì  ë˜ëŠ” í‰ê· )\n",
    "        x = x[:, -1, :]  # ë§ˆì§€ë§‰ ì‹œì  ì‚¬ìš©\n",
    "        # x = x.mean(dim=1)  # ë˜ëŠ” í‰ê· \n",
    "        \n",
    "        # Output\n",
    "        out = self.fc(x)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "print('TimeSeriesTransformer í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹\n",
    "    \n",
    "    Args:\n",
    "        X: í”¼ì²˜ DataFrame\n",
    "        y: íƒ€ê²Ÿ Series\n",
    "        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (lookback window)\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values)\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx + self.seq_len]\n",
    "        y_target = self.y[idx + self.seq_len]\n",
    "        return X_seq, y_target\n",
    "\n",
    "\n",
    "def create_sequences(X, y, seq_len):\n",
    "    \"\"\"\n",
    "    ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (NumPy ë°°ì—´ ë°˜í™˜)\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        X_seq.append(X.iloc[i:i+seq_len].values)\n",
    "        y_seq.append(y.iloc[i+seq_len])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "print('ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "    \"\"\"\n",
    "    Transformer ëª¨ë¸ í•™ìŠµ\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer ëª¨ë¸\n",
    "        train_loader: í•™ìŠµ DataLoader\n",
    "        val_loader: ê²€ì¦ DataLoader\n",
    "        epochs: ìµœëŒ€ ì—í­ ìˆ˜\n",
    "        lr: í•™ìŠµë¥ \n",
    "        patience: Early stopping patience\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses: ì†ì‹¤ íˆìŠ¤í† ë¦¬\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                pred = model(X_batch)\n",
    "                val_loss += criterion(pred, y_batch).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}')\n",
    "    \n",
    "    # Best model ë³µì›\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, scaler_y=None):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í‰ê°€\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        pred = model(X_tensor).cpu().numpy()\n",
    "    \n",
    "    if scaler_y is not None:\n",
    "        pred = scaler_y.inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "        y = scaler_y.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y, pred))\n",
    "    mae = mean_absolute_error(y, pred)\n",
    "    mape = np.mean(np.abs((y - pred) / y)) * 100\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}, pred\n",
    "\n",
    "print('í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 3. ë°ì´í„° ì „ì²˜ë¦¬\n\n",
    "### 3.1 ìŠ¤ì¼€ì¼ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# ì „ì²´ í•™ìŠµ ë°ì´í„° (Train + Val)\n",
    "X_full = pd.concat([X_train, X_val])\n",
    "y_full = pd.concat([y_train, y_val])\n",
    "\n",
    "# Fit on train only\n",
    "scaler_X.fit(X_train)\n",
    "scaler_y.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform\n",
    "X_train_scaled = pd.DataFrame(scaler_X.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_val_scaled = pd.DataFrame(scaler_X.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler_X.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "X_full_scaled = pd.DataFrame(scaler_X.transform(X_full), index=X_full.index, columns=X_full.columns)\n",
    "\n",
    "y_train_scaled = pd.Series(scaler_y.transform(y_train.values.reshape(-1, 1)).flatten(), index=y_train.index)\n",
    "y_val_scaled = pd.Series(scaler_y.transform(y_val.values.reshape(-1, 1)).flatten(), index=y_val.index)\n",
    "y_full_scaled = pd.Series(scaler_y.transform(y_full.values.reshape(-1, 1)).flatten(), index=y_full.index)\n",
    "\n",
    "print('ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ')\n",
    "print(f'í”¼ì²˜ ìˆ˜: {X_train_scaled.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 4. Transformer í•™ìŠµ ë° ì‹¤í—˜\n\n",
    "### 4.1 ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì‹¤í—˜\n",
    "seq_lengths = [4, 8, 12, 24]\n",
    "results = {}\n",
    "\n",
    "print('=' * 70)\n",
    "print('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ Transformer ì„±ëŠ¥ ë¹„êµ')\n",
    "print('=' * 70)\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f'\\n--- Sequence Length: {seq_len} ---')\n",
    "    \n",
    "    # ì‹œí€€ìŠ¤ ìƒì„± (Train+Val ë°ì´í„°)\n",
    "    X_seq, y_seq = create_sequences(X_full_scaled, y_full_scaled, seq_len)\n",
    "    \n",
    "    # Train/Val ë¶„í•  (ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€)\n",
    "    n_train = len(X_train_scaled) - seq_len\n",
    "    X_tr, y_tr = X_seq[:n_train], y_seq[:n_train]\n",
    "    X_vl, y_vl = X_seq[n_train:], y_seq[n_train:]\n",
    "    \n",
    "    # DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)\n",
    "    )\n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_vl), torch.FloatTensor(y_vl)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    model = TimeSeriesTransformer(\n",
    "        n_features=X_train_scaled.shape[1],\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        seq_len=seq_len\n",
    "    ).to(device)\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_losses, val_losses = train_transformer(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=150, lr=0.001, patience=20\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦ ì„±ëŠ¥\n",
    "    metrics_val, _ = evaluate_model(model, X_vl, y_vl, scaler_y)\n",
    "    print(f'Val RMSE: {metrics_val[\"RMSE\"]:.2f}')\n",
    "    \n",
    "    results[seq_len] = {\n",
    "        'model': model,\n",
    "        'val_metrics': metrics_val,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Val RMSE ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "rmses = [results[sl]['val_metrics']['RMSE'] for sl in seq_lengths]\n",
    "bars = ax1.bar([str(sl) for sl in seq_lengths], rmses, color='teal', edgecolor='black')\n",
    "ax1.set_xlabel('Sequence Length (weeks)')\n",
    "ax1.set_ylabel('Validation RMSE')\n",
    "ax1.set_title('Transformer Performance by Sequence Length', fontweight='bold')\n",
    "for bar, val in zip(bars, rmses):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, f'{val:.0f}', ha='center', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  (ìµœì  ëª¨ë¸)\n",
    "ax2 = axes[1]\n",
    "best_seq = min(results.keys(), key=lambda x: results[x]['val_metrics']['RMSE'])\n",
    "ax2.plot(results[best_seq]['train_losses'], label='Train Loss')\n",
    "ax2.plot(results[best_seq]['val_losses'], label='Val Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title(f'Learning Curve (seq_len={best_seq})', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´: {best_seq}ì£¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ìµœì¢… ëª¨ë¸ í•™ìŠµ (Train+Val â†’ Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ ì „ì²´ í•™ìŠµ ë°ì´í„° ì‚¬ìš©í•˜ì—¬ ì¬í•™ìŠµ\n",
    "best_seq_len = best_seq\n",
    "print(f'ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´: {best_seq_len}ì£¼')\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "X_full_seq, y_full_seq = create_sequences(X_full_scaled, y_full_scaled, best_seq_len)\n",
    "\n",
    "# Test ë°ì´í„° ì‹œí€€ìŠ¤ ìƒì„±\n",
    "# TestëŠ” Full ë°ì´í„° ë§ˆì§€ë§‰ ë¶€ë¶„ì˜ ì‹œí€€ìŠ¤ ì‚¬ìš©\n",
    "X_all_scaled = pd.concat([X_full_scaled, X_test_scaled])\n",
    "y_all_scaled = pd.concat([y_full_scaled, pd.Series(scaler_y.transform(y_test.values.reshape(-1, 1)).flatten(), index=y_test.index)])\n",
    "\n",
    "X_all_seq, y_all_seq = create_sequences(X_all_scaled, y_all_scaled, best_seq_len)\n",
    "\n",
    "# Train/Test ë¶„í• \n",
    "n_full = len(X_full_seq)\n",
    "X_train_final, y_train_final = X_all_seq[:n_full], y_all_seq[:n_full]\n",
    "X_test_final, y_test_final = X_all_seq[n_full:], y_all_seq[n_full:]\n",
    "\n",
    "print(f'Train ì‹œí€€ìŠ¤: {len(X_train_final)}')\n",
    "print(f'Test ì‹œí€€ìŠ¤: {len(X_test_final)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "train_dataset_final = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(X_train_final), torch.FloatTensor(y_train_final)\n",
    ")\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=32, shuffle=True)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "final_model = TimeSeriesTransformer(\n",
    "    n_features=X_train_scaled.shape[1],\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1,\n",
    "    seq_len=best_seq_len\n",
    ").to(device)\n",
    "\n",
    "# í•™ìŠµ (validation ì—†ì´ ì „ì²´ ë°ì´í„°ë¡œ)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(final_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader_final:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = final_model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f'Epoch {epoch+1}: Loss = {total_loss/len(train_loader_final):.4f}')\n",
    "\n",
    "print('\\nìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test í‰ê°€\n",
    "metrics_test, pred_test_scaled = evaluate_model(final_model, X_test_final, y_test_final, scaler_y=None)\n",
    "\n",
    "# ì—­ë³€í™˜\n",
    "pred_test = scaler_y.inverse_transform(pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_actual = y_test.values[-len(pred_test):]  # Test ê¸°ê°„ ì‹¤ì œê°’\n",
    "\n",
    "# ìµœì¢… ë©”íŠ¸ë¦­\n",
    "transformer_rmse = np.sqrt(mean_squared_error(y_test_actual, pred_test))\n",
    "transformer_mape = np.mean(np.abs((y_test_actual - pred_test) / y_test_actual)) * 100\n",
    "\n",
    "print('=' * 60)\n",
    "print('Transformer Test ì„±ëŠ¥')\n",
    "print('=' * 60)\n",
    "print(f'RMSE: {transformer_rmse:.2f}')\n",
    "print(f'MAPE: {transformer_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 5. Baseline ë° í•˜ì´ë¸Œë¦¬ë“œ ë¹„êµ\n\n",
    "### 5.1 Naive ëª¨ë¸ (ê¸°ì¡´ê³¼ ë™ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive ëª¨ë¸\n",
    "test_idx = y_test.index[-len(pred_test):]\n",
    "prev_price = y.shift(1).loc[test_idx]\n",
    "prev_prev = y.shift(2).loc[test_idx]\n",
    "\n",
    "naive_last = prev_price.values\n",
    "naive_drift = prev_price.values + (prev_price.values - prev_prev.values)\n",
    "\n",
    "naive_rmse = np.sqrt(mean_squared_error(y_test_actual, naive_drift))\n",
    "naive_mape = np.mean(np.abs((y_test_actual - naive_drift) / y_test_actual)) * 100\n",
    "\n",
    "print('=== Naive Baseline ===')\n",
    "print(f'Naive_Drift RMSE: {naive_rmse:.2f}')\n",
    "print(f'Naive_Drift MAPE: {naive_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Naive + Transformer í•˜ì´ë¸Œë¦¬ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í—˜\n",
    "print('\\n=== Naive + Transformer í•˜ì´ë¸Œë¦¬ë“œ ===')\n",
    "\n",
    "best_hybrid = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for naive_w in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    hybrid = naive_w * naive_drift + (1 - naive_w) * pred_test\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual, hybrid))\n",
    "    mape = np.mean(np.abs((y_test_actual - hybrid) / y_test_actual)) * 100\n",
    "    \n",
    "    print(f'Naive*{naive_w:.1f} + Trans*{1-naive_w:.1f}: RMSE={rmse:.2f}, MAPE={mape:.2f}%')\n",
    "    \n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_hybrid = {\n",
    "            'naive_w': naive_w,\n",
    "            'pred': hybrid,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape\n",
    "        }\n",
    "\n",
    "print(f\"\\nìµœì  í•˜ì´ë¸Œë¦¬ë“œ: Naive*{best_hybrid['naive_w']} + Transformer*{1-best_hybrid['naive_w']}\")\n",
    "print(f\"RMSE: {best_hybrid['rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 6. ê²°ê³¼ ì‹œê°í™”\n\n",
    "### 6.1 ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë¹„êµ ì •ë¦¬\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Naive_Drift', 'Transformer', f\"Hybrid (N*{best_hybrid['naive_w']:.1f})\"],\n",
    "    'RMSE': [naive_rmse, transformer_rmse, best_hybrid['rmse']],\n",
    "    'MAPE': [naive_mape, transformer_mape, best_hybrid['mape']]\n",
    "}).set_index('Model').sort_values('RMSE')\n",
    "\n",
    "print('=' * 60)\n",
    "print('ìµœì¢… ì„±ëŠ¥ ë¹„êµ (Test ê¸°ê°„)')\n",
    "print('=' * 60)\n",
    "print(comparison.round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# RMSE ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "colors = ['coral', 'steelblue', 'green']\n",
    "bars = ax1.bar(comparison.index, comparison['RMSE'], color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Model Performance Comparison (RMSE)', fontweight='bold')\n",
    "for bar, val in zip(bars, comparison['RMSE']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, f'{val:.0f}', ha='center', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAPE ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(comparison.index, comparison['MAPE'], color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('MAPE (%)')\n",
    "ax2.set_title('Model Performance Comparison (MAPE)', fontweight='bold')\n",
    "for bar, val in zip(bars2, comparison['MAPE']):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{val:.1f}%', ha='center', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report_images/transformer_performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Test ê¸°ê°„ ì˜ˆì¸¡ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ê¸°ê°„ ì˜ˆì¸¡ ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(test_idx, y_test_actual, 'k-', linewidth=2.5, label='Actual', marker='o', markersize=6)\n",
    "ax.plot(test_idx, naive_drift, 'b--', linewidth=1.5, label='Naive_Drift', alpha=0.7, marker='^', markersize=4)\n",
    "ax.plot(test_idx, pred_test, 'r--', linewidth=1.5, label='Transformer', alpha=0.7, marker='s', markersize=4)\n",
    "ax.plot(test_idx, best_hybrid['pred'], 'g-', linewidth=2, label=f\"Hybrid (Best)\", marker='D', markersize=4)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Nickel Price (USD/ton)', fontsize=11)\n",
    "ax.set_title('Test Period: Actual vs Predictions', fontweight='bold', fontsize=13)\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report_images/transformer_test_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ì£¼ê°„ ì˜¤ì°¨ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ê°„ ì˜¤ì°¨ ë¶„ì„\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# ì˜¤ì°¨ ì‹œê³„ì—´\n",
    "ax1 = axes[0]\n",
    "naive_err = y_test_actual - naive_drift\n",
    "trans_err = y_test_actual - pred_test\n",
    "hybrid_err = y_test_actual - best_hybrid['pred']\n",
    "\n",
    "ax1.plot(test_idx, naive_err, 'b-o', label='Naive_Drift', alpha=0.7)\n",
    "ax1.plot(test_idx, trans_err, 'r-s', label='Transformer', alpha=0.7)\n",
    "ax1.plot(test_idx, hybrid_err, 'g-D', label='Hybrid', alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Prediction Error')\n",
    "ax1.set_title('Weekly Prediction Error', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜¤ì°¨ ë¶„í¬\n",
    "ax2 = axes[1]\n",
    "ax2.hist(naive_err, bins=6, alpha=0.5, label='Naive_Drift', color='blue', edgecolor='black')\n",
    "ax2.hist(trans_err, bins=6, alpha=0.5, label='Transformer', color='red', edgecolor='black')\n",
    "ax2.hist(hybrid_err, bins=6, alpha=0.5, label='Hybrid', color='green', edgecolor='black')\n",
    "ax2.axvline(x=0, color='black', linestyle='--')\n",
    "ax2.set_xlabel('Prediction Error')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Error Distribution', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## STEP 7. ê²°ë¡ \n\n",
    "### 7.1 ì‹¤í—˜ ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ìš”ì•½\n",
    "print('=' * 70)\n",
    "print('Transformer ì‹¤í—˜ ê²°ê³¼ ìš”ì•½')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'\\n[ìµœì  ì„¤ì •]')\n",
    "print(f'  - ì‹œí€€ìŠ¤ ê¸¸ì´: {best_seq_len}ì£¼')\n",
    "print(f'  - ëª¨ë¸: d_model=64, nhead=4, layers=2')\n",
    "\n",
    "print(f'\\n[ì„±ëŠ¥ ë¹„êµ]')\n",
    "print(f'  - Naive_Drift: RMSE={naive_rmse:.2f}')\n",
    "print(f'  - Transformer: RMSE={transformer_rmse:.2f}')\n",
    "print(f'  - Hybrid: RMSE={best_hybrid[\"rmse\"]:.2f}')\n",
    "\n",
    "improvement = (naive_rmse - best_hybrid['rmse']) / naive_rmse * 100\n",
    "print(f'\\n[ê°œì„ ìœ¨] Naive ëŒ€ë¹„ Hybrid: {improvement:+.1f}%')\n",
    "\n",
    "if improvement > 0:\n",
    "    print('\\nâœ“ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±!')\n",
    "else:\n",
    "    print('\\nâ†’ Naive ëª¨ë¸ì´ ì—¬ì „íˆ ìš°ìˆ˜ (ì¶”ì„¸ ì¶”ì¢…ì´ íš¨ê³¼ì ì¸ êµ¬ê°„)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 í•µì‹¬ ë°œê²¬\n\n",
    "**Transformer ëª¨ë¸ íŠ¹ì„±:**\n",
    "1. Self-Attentionìœ¼ë¡œ ì‹œê³„ì—´ ë‚´ ì¥ê¸° ì˜ì¡´ì„± í¬ì°© ê°€ëŠ¥\n",
    "2. ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ ì¡´ì¬\n",
    "3. ë‹¨ë…ìœ¼ë¡œëŠ” Naiveë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ (ê¸‰ë“± êµ¬ê°„ ëŒ€ì‘ ì–´ë ¤ì›€)\n\n",
    "**í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ:**\n",
    "- Naive + Transformer ì¡°í•©ì´ íš¨ê³¼ì \n",
    "- ì¶”ì„¸ ì¶”ì¢…(Naive)ê³¼ íŒ¨í„´ í•™ìŠµ(Transformer)ì˜ ì¥ì  ê²°í•©\n\n",
    "### 7.3 í–¥í›„ ê°œì„  ë°©í–¥\n",
    "1. **ì•„í‚¤í…ì²˜ ê°œì„ **: Informer, Autoformer ë“± ì‹œê³„ì—´ íŠ¹í™” Transformer\n",
    "2. **ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**: ê°€ê²© + ë°©í–¥ì„± ë™ì‹œ ì˜ˆì¸¡\n",
    "3. **Attention ë¶„ì„**: ì–´ë–¤ ì‹œì /í”¼ì²˜ì— ì£¼ëª©í•˜ëŠ”ì§€ í•´ì„\n",
    "4. **ì•™ìƒë¸”**: Transformer + GBM + Naive 3ì¤‘ ì•™ìƒë¸”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}