{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  ë‹ˆì¼ˆ ê°€ê²© ì˜ˆì¸¡ - ë”¥ëŸ¬ë‹ ì‹¬í™” ì‹¤í—˜\n",
                "\n",
                "## 1. ë°°ê²½ ë° ëª©ì \n",
                "\n",
                "### 1.1 sparta2 ë”¥ëŸ¬ë‹ ì‹¤í—˜ ê²°ê³¼\n",
                "| ëª¨ë¸ | Val RMSE | Test RMSE | ê²°ê³¼ |\n",
                "|------|----------|-----------|------|\n",
                "| LSTM | ~461 | ~1957 | **ê·¹ì‹¬í•œ ê³¼ì í•©** |\n",
                "| Transformer | ìœ ì‚¬ | ìœ ì‚¬ | **ê·¹ì‹¬í•œ ê³¼ì í•©** |\n",
                "\n",
                "### 1.2 ì‹¤íŒ¨ ì›ì¸ ë¶„ì„\n",
                "- Train/Val ê¸°ê°„: í‰ê· íšŒê·€ íŒ¨í„´ â†’ DLì´ ì™„ë²½íˆ í•™ìŠµ\n",
                "- Test ê¸°ê°„: ì¼ë°©ì  ê¸‰ë“± â†’ í•™ìŠµí•œ íŒ¨í„´ê³¼ ì •ë°˜ëŒ€\n",
                "- **ê²°ë¡ **: DL ë‹¨ë…ì€ ì‹œì¥ êµ¬ì¡° ë³€í™”ì— ì·¨ì•½\n",
                "\n",
                "### 1.3 í•µì‹¬ ì§ˆë¬¸\n",
                "> **\"DLì„ ML/Naiveì™€ ê²°í•©í•˜ë©´ ê³¼ì í•©ì„ ì™„í™”í•˜ê³  ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥í•œê°€?\"**\n",
                "\n",
                "### 1.4 ê°€ì„¤\n",
                "| ê°€ì„¤ | ì„¤ëª… | ê²€ì¦ ë°©ë²• |\n",
                "|------|------|----------|\n",
                "| H1 | DL ê³¼ì í•© ë°©ì§€ ì„¤ê³„ë¡œ ë‹¨ë… ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥ | Dropout, ì¡°ê¸°ì¢…ë£Œ, ì •ê·œí™” ê°•í™” |\n",
                "| H2 | Naive + DL ì•™ìƒë¸”ë¡œ ê³¼ì í•© ì™„í™” ê°€ëŠ¥ | ê°€ì¤‘ í‰ê·  (Naive ì¤‘ì‹¬) |\n",
                "| H3 | ML + DL ì•™ìƒë¸”ì´ ML ë‹¨ë…ë³´ë‹¤ íš¨ê³¼ì  | GB + LSTM/Transformer ê²°í•© |\n",
                "| H4 | Quantile Regressionìœ¼ë¡œ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ê°€ëŠ¥ | ì˜ˆì¸¡ êµ¬ê°„ ì»¤ë²„ë¦¬ì§€ í™•ì¸ |\n",
                "\n",
                "### 1.5 ì‹¤í—˜ ì„¤ê³„\n",
                "```\n",
                "Step 0: ê¸°ì¤€ì„  í™•ì¸ (Naive, ML, Hybrid)\n",
                "    â†“\n",
                "Step 1: [H1] DL ê³¼ì í•© ë°©ì§€ ì„¤ê³„ ì ìš©\n",
                "    â†“\n",
                "Step 2: [H2] Naive + DL ì•™ìƒë¸”\n",
                "    â†“\n",
                "Step 3: [H3] ML + DL ì•™ìƒë¸”\n",
                "    â†“\n",
                "Step 4: [H4] Quantile Regression\n",
                "    â†“\n",
                "ê²°ë¡ : DL í™œìš© ê°€ì¹˜ í‰ê°€\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# í™˜ê²½ ì„¤ì •\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import shap\n",
                "import xgboost as xgb\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.linear_model import QuantileRegressor\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import os\n",
                "OUTPUT_DIR = 'output'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "CONFIG = {\n",
                "    'data_file': 'data_weekly_260120.csv',\n",
                "    'target_col': 'Com_LME_Ni_Cash',\n",
                "    'val_start': '2025-08-04',\n",
                "    'val_end': '2025-10-20',\n",
                "    'test_start': '2025-10-27',\n",
                "    'test_end': '2026-01-12',\n",
                "    'top_n_features': 20,\n",
                "    'exclude_lme_index': True,\n",
                "    'random_seed': 42,\n",
                "    'seq_len': 12,\n",
                "}\n",
                "\n",
                "def eval_metrics(y_true, y_pred, name=''):\n",
                "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-8))) * 100\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    return {'Model': name, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
                "\n",
                "np.random.seed(CONFIG['random_seed'])\n",
                "torch.manual_seed(CONFIG['random_seed'])\n",
                "\n",
                "print('='*60)\n",
                "print('ğŸ§  ë”¥ëŸ¬ë‹ ì‹¬í™” ì‹¤í—˜')\n",
                "print(f'Device: {device}')\n",
                "print('='*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. ë°ì´í„° ì¤€ë¹„ (sparta2 ë™ì¼)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (sparta2 ë™ì¼)\n",
                "df_raw = pd.read_csv(CONFIG['data_file'])\n",
                "df_raw['dt'] = pd.to_datetime(df_raw['dt'])\n",
                "df_raw = df_raw.set_index('dt').sort_index()\n",
                "\n",
                "def filter_cols(columns):\n",
                "    target = CONFIG['target_col']\n",
                "    metals = ['Gold', 'Silver', 'Iron', 'Steel', 'Copper', 'Aluminum', 'Zinc', 'Nickel', 'Lead', 'Tin', 'Uranium']\n",
                "    filtered = [target]\n",
                "    for col in columns:\n",
                "        if any(x in col for x in ['Idx_', 'Bonds_', 'EX_']):\n",
                "            filtered.append(col)\n",
                "        elif 'Com_LME' in col:\n",
                "            filtered.append(col)\n",
                "        elif any(m in col for m in metals):\n",
                "            filtered.append(col)\n",
                "    return sorted(list(set(filtered)))\n",
                "\n",
                "df = df_raw[filter_cols(df_raw.columns)].copy().ffill().dropna()\n",
                "y = df[CONFIG['target_col']]\n",
                "X = df.drop(columns=[CONFIG['target_col']]).shift(1)\n",
                "valid_idx = X.dropna().index.intersection(y.dropna().index)\n",
                "X, y = X.loc[valid_idx], y.loc[valid_idx]\n",
                "\n",
                "if CONFIG['exclude_lme_index']:\n",
                "    X = X.drop(columns=[c for c in X.columns if 'Com_LME_Index' in c], errors='ignore')\n",
                "\n",
                "train_mask = X.index < CONFIG['val_start']\n",
                "val_mask = (X.index >= CONFIG['val_start']) & (X.index <= CONFIG['val_end'])\n",
                "test_mask = (X.index >= CONFIG['test_start']) & (X.index <= CONFIG['test_end'])\n",
                "\n",
                "X_train_all, y_train = X[train_mask], y[train_mask]\n",
                "X_val_all, y_val = X[val_mask], y[val_mask]\n",
                "X_test_all, y_test = X[test_mask], y[test_mask]\n",
                "\n",
                "# SHAP í”¼ì²˜ ì„ íƒ\n",
                "model_shap = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "model_shap.fit(X_train_all, y_train)\n",
                "shap_values = shap.TreeExplainer(model_shap).shap_values(X_train_all)\n",
                "feat_imp = pd.DataFrame({'feature': X_train_all.columns, 'importance': np.abs(shap_values).mean(axis=0)})\n",
                "selected_features = feat_imp.sort_values('importance', ascending=False).head(CONFIG['top_n_features'])['feature'].tolist()\n",
                "\n",
                "X_train, X_val, X_test = X_train_all[selected_features], X_val_all[selected_features], X_test_all[selected_features]\n",
                "\n",
                "print(f'âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ')\n",
                "print(f'   Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ (DLìš©)\n",
                "SEQ_LEN = CONFIG['seq_len']\n",
                "\n",
                "def create_sequences(X, y, seq_len):\n",
                "    X_seq, y_seq = [], []\n",
                "    for i in range(len(X) - seq_len):\n",
                "        X_seq.append(X[i:i+seq_len])\n",
                "        y_seq.append(y[i+seq_len])\n",
                "    return np.array(X_seq), np.array(y_seq)\n",
                "\n",
                "scaler_X, scaler_y = StandardScaler(), StandardScaler()\n",
                "X_all = pd.concat([X_train, X_val, X_test])\n",
                "y_all = pd.concat([y_train, y_val, y_test])\n",
                "\n",
                "X_scaled = scaler_X.fit_transform(X_all)\n",
                "y_scaled = scaler_y.fit_transform(y_all.values.reshape(-1, 1)).flatten()\n",
                "\n",
                "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LEN)\n",
                "\n",
                "n_train = len(X_train) - SEQ_LEN\n",
                "n_val = len(X_val)\n",
                "n_test = len(X_test)\n",
                "\n",
                "X_train_seq, y_train_seq = X_seq[:n_train], y_seq[:n_train]\n",
                "X_val_seq, y_val_seq = X_seq[n_train:n_train+n_val], y_seq[n_train:n_train+n_val]\n",
                "X_test_seq, y_test_seq = X_seq[n_train+n_val:n_train+n_val+n_test], y_seq[n_train+n_val:n_train+n_val+n_test]\n",
                "\n",
                "n_features = X_train_seq.shape[2]\n",
                "print(f'   Seq Train: {len(X_train_seq)} | Val: {len(X_val_seq)} | Test: {len(X_test_seq)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Step 0: ê¸°ì¤€ì„  í™•ì¸"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ê¸°ì¤€ì„  ëª¨ë¸ë“¤\n",
                "prev_test = y.shift(1).loc[y_test.index].values\n",
                "prev2_test = y.shift(2).loc[y_test.index].values\n",
                "naive_drift_test = prev_test + (prev_test - prev2_test)\n",
                "\n",
                "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
                "gb.fit(X_train, y_train)\n",
                "gb_pred_test = gb.predict(X_test)\n",
                "\n",
                "hybrid_test = 0.8 * naive_drift_test + 0.2 * gb_pred_test\n",
                "\n",
                "baseline_results = [\n",
                "    eval_metrics(y_test, naive_drift_test, 'Naive_Drift'),\n",
                "    eval_metrics(y_test, gb_pred_test, 'GradientBoosting'),\n",
                "    eval_metrics(y_test, hybrid_test, 'Hybrid(0.8+0.2) [Best]'),\n",
                "]\n",
                "baseline_df = pd.DataFrame(baseline_results).sort_values('RMSE')\n",
                "\n",
                "print('ğŸ“Š Step 0: ê¸°ì¤€ì„  (Test ê¸°ê°„)')\n",
                "print('='*50)\n",
                "display(baseline_df.round(2))\n",
                "\n",
                "BEST_BASELINE_RMSE = baseline_df.iloc[0]['RMSE']\n",
                "print(f'\\nğŸ¯ ëª©í‘œ: RMSE < {BEST_BASELINE_RMSE:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Step 1: [H1] DL ê³¼ì í•© ë°©ì§€ ì„¤ê³„\n",
                "\n",
                "**ê°€ì„¤**: ê°•í•œ ì •ê·œí™”ë¡œ DL ë‹¨ë… ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥  \n",
                "**ê·¼ê±°**: ê¸°ì¡´ DLì€ Train íŒ¨í„´ì— ê³¼ì í•©  \n",
                "**ë°©ë²•**: Dropout ê°•í™”, ëª¨ë¸ ì¶•ì†Œ, ì¡°ê¸° ì¢…ë£Œ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 ëª¨ë¸ ì •ì˜ (ê°•í•œ ì •ê·œí™” ë²„ì „)\n",
                "class LSTMRegularized(nn.Module):\n",
                "    def __init__(self, n_feat, hidden=32, layers=1, dropout=0.3):  # ì¶•ì†Œëœ êµ¬ì¡°\n",
                "        super().__init__()\n",
                "        self.lstm = nn.LSTM(n_feat, hidden, layers, batch_first=True, dropout=0 if layers==1 else dropout)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.fc = nn.Linear(hidden, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        out, _ = self.lstm(x)\n",
                "        return self.fc(self.dropout(out[:, -1])).squeeze(-1)\n",
                "\n",
                "class TransformerRegularized(nn.Module):\n",
                "    def __init__(self, n_feat, d_model=32, nhead=2, layers=1, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.proj = nn.Linear(n_feat, d_model)\n",
                "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=64, dropout=dropout, batch_first=True)\n",
                "        self.enc = nn.TransformerEncoder(enc_layer, layers)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.fc = nn.Linear(d_model, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.proj(x)\n",
                "        x = self.enc(x)\n",
                "        return self.fc(self.dropout(x[:, -1])).squeeze(-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.2 í•™ìŠµ í•¨ìˆ˜\n",
                "def train_dl(model, X_tr, y_tr, X_vl, y_vl, epochs=80, lr=0.001, patience=15):\n",
                "    model = model.to(device)\n",
                "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # L2 ì •ê·œí™”\n",
                "    loss_fn = nn.MSELoss()\n",
                "    \n",
                "    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)), batch_size=32, shuffle=True)\n",
                "    val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_vl), torch.FloatTensor(y_vl)), batch_size=32)\n",
                "    \n",
                "    best_loss, best_state, wait = np.inf, None, 0\n",
                "    \n",
                "    for ep in range(epochs):\n",
                "        model.train()\n",
                "        for xb, yb in train_loader:\n",
                "            xb, yb = xb.to(device), yb.to(device)\n",
                "            opt.zero_grad()\n",
                "            loss_fn(model(xb), yb).backward()\n",
                "            opt.step()\n",
                "        \n",
                "        model.eval()\n",
                "        val_loss = sum(loss_fn(model(xb.to(device)), yb.to(device)).item() for xb, yb in val_loader) / len(val_loader)\n",
                "        \n",
                "        if val_loss < best_loss:\n",
                "            best_loss, best_state, wait = val_loss, {k: v.cpu().clone() for k, v in model.state_dict().items()}, 0\n",
                "        else:\n",
                "            wait += 1\n",
                "            if wait >= patience: break\n",
                "    \n",
                "    if best_state: model.load_state_dict(best_state)\n",
                "    return model\n",
                "\n",
                "def predict_dl(model, X):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        pred = model(torch.FloatTensor(X).to(device)).cpu().numpy()\n",
                "    return scaler_y.inverse_transform(pred.reshape(-1, 1)).flatten()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.3 LSTM & Transformer í•™ìŠµ\n",
                "print('ğŸ” H1: DL ê³¼ì í•© ë°©ì§€ ì„¤ê³„')\n",
                "print('-'*40)\n",
                "\n",
                "print('  LSTM í•™ìŠµ (ì •ê·œí™” ê°•í™”)...')\n",
                "lstm = train_dl(LSTMRegularized(n_features), X_train_seq, y_train_seq, X_val_seq, y_val_seq)\n",
                "lstm_pred = predict_dl(lstm, X_test_seq)\n",
                "\n",
                "print('  Transformer í•™ìŠµ (ì •ê·œí™” ê°•í™”)...')\n",
                "transformer = train_dl(TransformerRegularized(n_features), X_train_seq, y_train_seq, X_val_seq, y_val_seq)\n",
                "trans_pred = predict_dl(transformer, X_test_seq)\n",
                "\n",
                "y_test_raw = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
                "\n",
                "h1_lstm = eval_metrics(y_test_raw, lstm_pred, 'LSTM (Regularized)')\n",
                "h1_trans = eval_metrics(y_test_raw, trans_pred, 'Transformer (Regularized)')\n",
                "\n",
                "print(f'\\nğŸ“Š H1 ê²°ê³¼:')\n",
                "print(f'  LSTM: RMSE={h1_lstm[\"RMSE\"]:.2f}')\n",
                "print(f'  Transformer: RMSE={h1_trans[\"RMSE\"]:.2f}')\n",
                "print(f'  (ê¸°ì¡´ sparta2 LSTM: ~1957)')\n",
                "\n",
                "H1_IMPROVEMENT = min(h1_lstm['RMSE'], h1_trans['RMSE']) < 1500"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Step 2: [H2] Naive + DL ì•™ìƒë¸”\n",
                "\n",
                "**ê°€ì„¤**: Naiveì˜ ì¶”ì„¸ ì¶”ì¢…ë ¥ì´ DL ê³¼ì í•©ì„ ë³´ì™„ ê°€ëŠ¥  \n",
                "**ê·¼ê±°**: Naive(0.8) + ML(0.2)ê°€ ì„±ê³µì ì´ì—ˆìŒ  \n",
                "**ë°©ë²•**: Naive(0.8) + DL(0.2) êµ¬ì„±"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.1 ê¸¸ì´ ë§ì¶”ê¸° (ì‹œí€€ìŠ¤ë¡œ ì¤„ì–´ë“  ë§Œí¼)\n",
                "test_len = min(len(lstm_pred), len(naive_drift_test))\n",
                "naive_aligned = naive_drift_test[-test_len:]\n",
                "lstm_aligned = lstm_pred[-test_len:]\n",
                "trans_aligned = trans_pred[-test_len:]\n",
                "y_test_aligned = y_test.values[-test_len:]\n",
                "\n",
                "print('ğŸ” H2: Naive + DL ì•™ìƒë¸”')\n",
                "print('-'*40)\n",
                "\n",
                "# Naive 0.8 + DL 0.2\n",
                "naive_lstm = 0.8 * naive_aligned + 0.2 * lstm_aligned\n",
                "naive_trans = 0.8 * naive_aligned + 0.2 * trans_aligned\n",
                "naive_dl_avg = 0.8 * naive_aligned + 0.1 * lstm_aligned + 0.1 * trans_aligned\n",
                "\n",
                "h2_results = [\n",
                "    eval_metrics(y_test_aligned, naive_lstm, 'Naive+LSTM (0.8+0.2)'),\n",
                "    eval_metrics(y_test_aligned, naive_trans, 'Naive+Transformer (0.8+0.2)'),\n",
                "    eval_metrics(y_test_aligned, naive_dl_avg, 'Naive+DL_Avg (0.8+0.2)'),\n",
                "]\n",
                "\n",
                "h2_df = pd.DataFrame(h2_results).sort_values('RMSE')\n",
                "print('\\nğŸ“Š H2 ê²°ê³¼:')\n",
                "display(h2_df.round(2))\n",
                "\n",
                "H2_BEST = h2_df.iloc[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Step 3: [H3] ML + DL ì•™ìƒë¸”\n",
                "\n",
                "**ê°€ì„¤**: MLê³¼ DLì˜ ì˜ˆì¸¡ ë‹¤ì–‘ì„±ì´ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬  \n",
                "**ê·¼ê±°**: ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•˜ë¯€ë¡œ ë³´ì™„ ê°€ëŠ¥  \n",
                "**ë°©ë²•**: GB + LSTM/Transformer ê°€ì¤‘ ê²°í•©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.1 ML + DL ì•™ìƒë¸”\n",
                "print('ğŸ” H3: ML + DL ì•™ìƒë¸”')\n",
                "print('-'*40)\n",
                "\n",
                "gb_aligned = gb_pred_test[-test_len:]\n",
                "\n",
                "# ë‹¤ì–‘í•œ ì¡°í•©\n",
                "ml_lstm = 0.7 * gb_aligned + 0.3 * lstm_aligned\n",
                "ml_trans = 0.7 * gb_aligned + 0.3 * trans_aligned\n",
                "ml_dl_avg = 0.6 * gb_aligned + 0.2 * lstm_aligned + 0.2 * trans_aligned\n",
                "\n",
                "# Naive + ML + DL 3-way\n",
                "three_way = 0.6 * naive_aligned + 0.2 * gb_aligned + 0.1 * lstm_aligned + 0.1 * trans_aligned\n",
                "\n",
                "h3_results = [\n",
                "    eval_metrics(y_test_aligned, ml_lstm, 'GB+LSTM (0.7+0.3)'),\n",
                "    eval_metrics(y_test_aligned, ml_trans, 'GB+Transformer (0.7+0.3)'),\n",
                "    eval_metrics(y_test_aligned, ml_dl_avg, 'GB+DL_Avg (0.6+0.4)'),\n",
                "    eval_metrics(y_test_aligned, three_way, '3-Way (N0.6+ML0.2+DL0.2)'),\n",
                "]\n",
                "\n",
                "h3_df = pd.DataFrame(h3_results).sort_values('RMSE')\n",
                "print('\\nğŸ“Š H3 ê²°ê³¼:')\n",
                "display(h3_df.round(2))\n",
                "\n",
                "H3_BEST = h3_df.iloc[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Step 4: [H4] Quantile Regression\n",
                "\n",
                "**ê°€ì„¤**: ì  ì˜ˆì¸¡ ì™¸ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ê°€ ì˜ì‚¬ê²°ì •ì— ìœ ìš©  \n",
                "**ê·¼ê±°**: ê¸‰ë“± ì‹œì¥ì—ì„œ ë¶ˆí™•ì‹¤ì„±ì´ í¼  \n",
                "**ë°©ë²•**: 10%, 25%, 50%, 75%, 90% ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7.1 Quantile Regression\n",
                "print('ğŸ” H4: Quantile Regression')\n",
                "print('-'*40)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train_sc = scaler.fit_transform(X_train)\n",
                "X_test_sc = scaler.transform(X_test)\n",
                "\n",
                "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
                "qr_preds = {}\n",
                "\n",
                "for q in quantiles:\n",
                "    qr = QuantileRegressor(quantile=q, alpha=0.1, solver='highs')\n",
                "    qr.fit(X_train_sc, y_train)\n",
                "    qr_preds[q] = qr.predict(X_test_sc)\n",
                "    print(f'  Q{int(q*100):02d} ì™„ë£Œ')\n",
                "\n",
                "# ì»¤ë²„ë¦¬ì§€ ê³„ì‚°\n",
                "coverage_80 = np.mean((y_test.values >= qr_preds[0.1]) & (y_test.values <= qr_preds[0.9])) * 100\n",
                "coverage_50 = np.mean((y_test.values >= qr_preds[0.25]) & (y_test.values <= qr_preds[0.75])) * 100\n",
                "\n",
                "median_rmse = np.sqrt(mean_squared_error(y_test, qr_preds[0.5]))\n",
                "\n",
                "print(f'\\nğŸ“Š H4 ê²°ê³¼:')\n",
                "print(f'  Median (Q50) RMSE: {median_rmse:.2f}')\n",
                "print(f'  80% êµ¬ê°„ (Q10-Q90) ì»¤ë²„ë¦¬ì§€: {coverage_80:.1f}% (ëª©í‘œ: 80%)')\n",
                "print(f'  50% êµ¬ê°„ (Q25-Q75) ì»¤ë²„ë¦¬ì§€: {coverage_50:.1f}% (ëª©í‘œ: 50%)')\n",
                "\n",
                "H4_VALID = coverage_80 >= 70  # 80%ì— ê·¼ì ‘í•˜ë©´ ìœ íš¨"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7.2 ì˜ˆì¸¡ êµ¬ê°„ ì‹œê°í™”\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "\n",
                "ax.plot(y_test.index, y_test.values, 'k-', label='Actual', lw=2)\n",
                "ax.plot(y_test.index, qr_preds[0.5], 'b--', label='Median (Q50)', lw=1.5)\n",
                "ax.fill_between(y_test.index, qr_preds[0.1], qr_preds[0.9], alpha=0.2, color='blue', label='80% PI')\n",
                "ax.fill_between(y_test.index, qr_preds[0.25], qr_preds[0.75], alpha=0.3, color='blue', label='50% PI')\n",
                "\n",
                "ax.set_title(f'Quantile Regression (80% Coverage: {coverage_80:.1f}%)', fontweight='bold')\n",
                "ax.set_ylabel('Nickel Price (USD/t)')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{OUTPUT_DIR}/quantile_regression.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. ìµœì¢… ê²°ê³¼ ë° ê²°ë¡ "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8.1 ì „ì²´ ê²°ê³¼ í†µí•©\n",
                "all_results = baseline_results + [h1_lstm, h1_trans] + h2_results + h3_results\n",
                "all_results.append(eval_metrics(y_test, qr_preds[0.5], 'QR Median'))\n",
                "\n",
                "final_df = pd.DataFrame(all_results).sort_values('RMSE')\n",
                "\n",
                "print('='*70)\n",
                "print('ğŸ“Š ì „ì²´ ê²°ê³¼ (Test ê¸°ê°„)')\n",
                "print('='*70)\n",
                "display(final_df[['Model', 'RMSE', 'MAPE', 'R2']].round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8.2 ê°€ì„¤ ê²€ì¦ ê²°ê³¼\n",
                "print('='*70)\n",
                "print('ğŸ“‹ ê°€ì„¤ ê²€ì¦ ê²°ê³¼')\n",
                "print('='*70)\n",
                "\n",
                "print(f'\\n[H1] DL ê³¼ì í•© ë°©ì§€ ì„¤ê³„')\n",
                "print(f'     ê²°ê³¼: {\"âœ… íš¨ê³¼ ìˆìŒ\" if H1_IMPROVEMENT else \"âŒ ì—¬ì „íˆ ê³¼ì í•©\"}')\n",
                "print(f'     LSTM: {h1_lstm[\"RMSE\"]:.2f}, Transformer: {h1_trans[\"RMSE\"]:.2f}')\n",
                "print(f'     (ê¸°ì¡´ sparta2 LSTM: ~1957)')\n",
                "\n",
                "print(f'\\n[H2] Naive + DL ì•™ìƒë¸”')\n",
                "naive_dl_improved = H2_BEST['RMSE'] < min(h1_lstm['RMSE'], h1_trans['RMSE'])\n",
                "print(f'     ê²°ê³¼: {\"âœ… ê°œì„ \" if naive_dl_improved else \"âŒ ê°œì„  ì—†ìŒ\"}')\n",
                "print(f'     Best: {H2_BEST[\"Model\"]} (RMSE={H2_BEST[\"RMSE\"]:.2f})')\n",
                "\n",
                "print(f'\\n[H3] ML + DL ì•™ìƒë¸”')\n",
                "ml_dl_improved = H3_BEST['RMSE'] < baseline_df[baseline_df['Model']=='GradientBoosting']['RMSE'].values[0]\n",
                "print(f'     ê²°ê³¼: {\"âœ… GB ëŒ€ë¹„ ê°œì„ \" if ml_dl_improved else \"âŒ GB ëŒ€ë¹„ ì—´ë“±\"}')\n",
                "print(f'     Best: {H3_BEST[\"Model\"]} (RMSE={H3_BEST[\"RMSE\"]:.2f})')\n",
                "\n",
                "print(f'\\n[H4] Quantile Regression')\n",
                "print(f'     ê²°ê³¼: {\"âœ… ìœ íš¨\" if H4_VALID else \"âš ï¸ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±\"}')\n",
                "print(f'     80% êµ¬ê°„ ì»¤ë²„ë¦¬ì§€: {coverage_80:.1f}%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8.3 ìµœì¢… ê²°ë¡ \n",
                "best = final_df.iloc[0]\n",
                "improvement = (BEST_BASELINE_RMSE - best['RMSE']) / BEST_BASELINE_RMSE * 100\n",
                "\n",
                "print('\\n' + '='*70)\n",
                "print('ğŸ† ìµœì¢… ê²°ë¡ ')\n",
                "print('='*70)\n",
                "\n",
                "print(f'\\nğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best[\"Model\"]}')\n",
                "print(f'   RMSE: {best[\"RMSE\"]:.2f}')\n",
                "print(f'   ê¸°ì¤€ì„  ëŒ€ë¹„: {improvement:+.2f}%')\n",
                "\n",
                "print(f'\\nğŸ“Œ DL í™œìš© ê°€ì¹˜ í‰ê°€:')\n",
                "if best['RMSE'] < BEST_BASELINE_RMSE:\n",
                "    print(f'   âœ… DL ê¸°ë°˜ ì•™ìƒë¸”ì´ ê¸°ì¤€ì„  ê°œì„ ì— ê¸°ì—¬')\n",
                "else:\n",
                "    print(f'   âš ï¸ DL ë‹¨ë…/ì•™ìƒë¸”ì´ Naive+ML Hybridë¥¼ ë„˜ì§€ ëª»í•¨')\n",
                "    print(f'   â†’ DLì€ ì‹œì¥ êµ¬ì¡° ë³€í™”ì— ì—¬ì „íˆ ì·¨ì•½')\n",
                "    print(f'   â†’ ì¶”ê°€ ê°€ì¹˜: Quantile Regressionìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”')\n",
                "\n",
                "print(f'\\nğŸ“Œ ì‹¤ë¬´ ê¶Œì¥:')\n",
                "print(f'   1ìˆœìœ„: Hybrid(Naive+ML) - ì•ˆì •ì ì´ê³  í•´ì„ ê°€ëŠ¥')\n",
                "print(f'   2ìˆœìœ„: Quantile Regression - ì˜ˆì¸¡ êµ¬ê°„ ì œê³µ')\n",
                "print(f'   DL: ë³´ì¡° ì•™ìƒë¸” ë©¤ë²„ë¡œë§Œ í™œìš©')\n",
                "print('='*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}